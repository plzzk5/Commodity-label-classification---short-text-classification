{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85f8ea02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import jieba\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73ba43d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>商品名称</th>\n",
       "      <th>一级分类</th>\n",
       "      <th>二级分类</th>\n",
       "      <th>三级分类</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**N蓝妹啤酒易拉罐3</td>\n",
       "      <td>酒类</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>啤酒</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>农夫水溶C100青皮桔445ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁</td>\n",
       "      <td>果蔬饮料</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N七匹狼（软灰）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N七匹狼（软红）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>农夫山泉水550ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>水</td>\n",
       "      <td>矿泉水</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               商品名称 一级分类 二级分类  三级分类\n",
       "0       **N蓝妹啤酒易拉罐3   酒类   啤酒    啤酒\n",
       "1  农夫水溶C100青皮桔445ml   饮料  果蔬汁  果蔬饮料\n",
       "2          N七匹狼（软灰）   烟类   香烟  软盒香烟\n",
       "3          N七匹狼（软红）   烟类   香烟  软盒香烟\n",
       "4        农夫山泉水550ml   饮料    水   矿泉水"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('/Users/mac/Desktop/2022春课件/时序/商品清单（new）.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d58883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>商品名称</th>\n",
       "      <th>一级分类</th>\n",
       "      <th>二级分类</th>\n",
       "      <th>三级分类</th>\n",
       "      <th>一级分类_整数</th>\n",
       "      <th>二级分类_整数</th>\n",
       "      <th>三级分类_整数</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**N蓝妹啤酒易拉罐3</td>\n",
       "      <td>酒类</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>农夫水溶C100青皮桔445ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁</td>\n",
       "      <td>果蔬饮料</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N七匹狼（软灰）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N七匹狼（软红）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>农夫山泉水550ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>水</td>\n",
       "      <td>矿泉水</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               商品名称 一级分类 二级分类  三级分类  一级分类_整数  二级分类_整数  三级分类_整数\n",
       "0       **N蓝妹啤酒易拉罐3   酒类   啤酒    啤酒        0        0        0\n",
       "1  农夫水溶C100青皮桔445ml   饮料  果蔬汁  果蔬饮料        1        1        1\n",
       "2          N七匹狼（软灰）   烟类   香烟  软盒香烟        2        2        2\n",
       "3          N七匹狼（软红）   烟类   香烟  软盒香烟        2        2        2\n",
       "4        农夫山泉水550ml   饮料    水   矿泉水        1        3        3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#标签映射为整数\n",
    "listType = data['一级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['一级分类_整数'] = data['一级分类'].map(com_map)\n",
    "\n",
    "listType = data['二级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['二级分类_整数'] = data['二级分类'].map(com_map)\n",
    "\n",
    "listType = data['三级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['三级分类_整数'] = data['三级分类'].map(com_map)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f743f4",
   "metadata": {},
   "source": [
    "## fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6d9e24",
   "metadata": {},
   "source": [
    "#### 处理为fasttext喜欢的格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "671a8345",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_t/zp2z4_4x4gg78j4gh86h6z5m0000gn/T/jieba.cache\n",
      "Loading model cost 1.034 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>商品名称</th>\n",
       "      <th>一级分类</th>\n",
       "      <th>二级分类</th>\n",
       "      <th>三级分类</th>\n",
       "      <th>一级分类_整数</th>\n",
       "      <th>二级分类_整数</th>\n",
       "      <th>三级分类_整数</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**N蓝妹啤酒易拉罐3</td>\n",
       "      <td>酒类</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[N, 蓝妹, 啤酒, 易拉罐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>农夫水溶C100青皮桔445ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁</td>\n",
       "      <td>果蔬饮料</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[农夫, 水溶, C100, 青皮, 桔, 445ml]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N七匹狼（软灰）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[N, 七匹狼, 软灰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N七匹狼（软红）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[N, 七匹狼, 软红]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>农夫山泉水550ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>水</td>\n",
       "      <td>矿泉水</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[农夫山泉, 水, 550ml]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               商品名称 一级分类 二级分类  三级分类  一级分类_整数  二级分类_整数  三级分类_整数  \\\n",
       "0       **N蓝妹啤酒易拉罐3   酒类   啤酒    啤酒        0        0        0   \n",
       "1  农夫水溶C100青皮桔445ml   饮料  果蔬汁  果蔬饮料        1        1        1   \n",
       "2          N七匹狼（软灰）   烟类   香烟  软盒香烟        2        2        2   \n",
       "3          N七匹狼（软红）   烟类   香烟  软盒香烟        2        2        2   \n",
       "4        农夫山泉水550ml   饮料    水   矿泉水        1        3        3   \n",
       "\n",
       "                        segment  \n",
       "0              [N, 蓝妹, 啤酒, 易拉罐]  \n",
       "1  [农夫, 水溶, C100, 青皮, 桔, 445ml]  \n",
       "2                  [N, 七匹狼, 软灰]  \n",
       "3                  [N, 七匹狼, 软红]  \n",
       "4              [农夫山泉, 水, 550ml]  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopkey = [w.strip() for w in codecs.open('/Users/mac/Downloads/呆萌的停用词表.txt', 'r').readlines()]\n",
    "data['segment'] = data['商品名称'].apply(lambda x:jieba.lcut(x))\n",
    "for i in range(len(data)):\n",
    "    words = data['segment'][i].copy()\n",
    "    for x in words:\n",
    "        if x in stopkey:\n",
    "            data['segment'][i].remove(x)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c2edce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label(pred):\n",
    "    index = np.argmax(pred[1])\n",
    "    label = int(pred[0][index][9:])\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb36908b",
   "metadata": {},
   "source": [
    "#### 一级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32d00b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data['segment']\n",
    "Y = data['一级分类_整数']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train = X_train.reset_index()['segment']\n",
    "Y_train = Y_train.reset_index()['一级分类_整数']\n",
    "X_test = X_test.reset_index()['segment']\n",
    "Y_test = Y_test.reset_index()['一级分类_整数']\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/train_semantic1.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        str1 = \" \".join(X_train[i])+\"\\t\"+\"__label__\"+str(Y_train[i])+'\\n'\n",
    "        f.write(str1)\n",
    "\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/test_semantic1.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        str1 = \" \".join(X_test[i])+\"\\t\"+\"__label__\"+str(Y_test[i])+'\\n'\n",
    "        f.write(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff04c88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.896260017809439\n",
      "平均f1-score: 0.8959551027723758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.97      0.93       123\n",
      "           1       0.95      0.92      0.94       481\n",
      "           2       0.94      0.94      0.94       195\n",
      "           3       0.96      0.93      0.94      1397\n",
      "           4       0.89      0.91      0.90       213\n",
      "           5       0.91      0.95      0.93       326\n",
      "           6       0.89      0.90      0.89       442\n",
      "           7       0.89      0.56      0.68       189\n",
      "           8       0.87      0.97      0.92       145\n",
      "           9       0.89      0.89      0.89       384\n",
      "          10       0.60      0.78      0.68        59\n",
      "          11       0.72      0.74      0.73       212\n",
      "          12       0.75      0.80      0.77        54\n",
      "          13       0.91      0.94      0.93        34\n",
      "          14       0.70      0.83      0.76        46\n",
      "          15       0.80      0.88      0.83        40\n",
      "          16       0.30      0.60      0.40         5\n",
      "          17       0.87      0.94      0.90       117\n",
      "          18       0.80      1.00      0.89        20\n",
      "          19       0.55      0.86      0.67         7\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.11      0.50      0.18         2\n",
      "          22       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           0.90      4492\n",
      "   macro avg       0.75      0.82      0.77      4492\n",
      "weighted avg       0.90      0.90      0.90      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "model = fasttext.train_supervised(input=\"/Users/mac/Desktop/2022春课件/时序/train_semantic1.txt\",lr=0.2, epoch=500, wordNgrams=2, dim=300,loss='softmax')\n",
    "Y_pre = []\n",
    "for i in range(len(X_test)):\n",
    "    r = model.predict(\" \".join(X_test[i]),k=2)\n",
    "    Y_pre.append(get_label(r))\n",
    "\n",
    "\n",
    "print('准确率', accuracy_score(Y_test, Y_pre))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_pre, average='weighted'))\n",
    "print(classification_report(Y_pre,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4de41b",
   "metadata": {},
   "source": [
    "二级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "110cc9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['二级分类_整数']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train = X_train.reset_index()['segment']\n",
    "Y_train = Y_train.reset_index()['二级分类_整数']\n",
    "X_test = X_test.reset_index()['segment']\n",
    "Y_test = Y_test.reset_index()['二级分类_整数']\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/train_semantic2.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        str1 = \" \".join(X_train[i])+\"\\t\"+\"__label__\"+str(Y_train[i])+'\\n'\n",
    "        f.write(str1)\n",
    "\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/test_semantic2.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        str1 = \" \".join(X_test[i])+\"\\t\"+\"__label__\"+str(Y_test[i])+'\\n'\n",
    "        f.write(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e90011e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.8118878005342832\n",
      "平均f1-score: 0.8134960144432843\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        54\n",
      "           1       0.86      0.76      0.81        89\n",
      "           2       0.96      0.88      0.92       201\n",
      "           3       0.92      0.98      0.95        48\n",
      "           4       0.94      0.88      0.91       133\n",
      "           5       0.76      0.78      0.77       130\n",
      "           6       0.86      0.93      0.90        61\n",
      "           7       0.90      0.87      0.88       109\n",
      "           8       0.92      0.94      0.93        50\n",
      "           9       0.88      0.88      0.88       216\n",
      "          10       0.88      0.82      0.84        60\n",
      "          11       0.78      0.87      0.82        46\n",
      "          12       0.78      0.90      0.83        50\n",
      "          13       0.74      0.64      0.69        70\n",
      "          14       0.93      0.90      0.91       183\n",
      "          15       0.77      0.89      0.83        95\n",
      "          16       1.00      1.00      1.00        12\n",
      "          17       0.83      0.96      0.89        25\n",
      "          18       0.68      0.61      0.65        88\n",
      "          19       0.67      0.47      0.55       134\n",
      "          20       0.70      0.72      0.71       144\n",
      "          21       0.97      0.42      0.59       142\n",
      "          22       0.85      0.89      0.87       131\n",
      "          23       0.55      0.57      0.56        28\n",
      "          24       0.54      0.78      0.64         9\n",
      "          25       0.82      0.86      0.84       118\n",
      "          26       0.56      0.68      0.61        57\n",
      "          27       0.40      1.00      0.57         4\n",
      "          28       0.90      0.97      0.93        58\n",
      "          29       0.77      0.91      0.83        22\n",
      "          30       0.89      0.89      0.89        28\n",
      "          31       0.50      0.75      0.60         4\n",
      "          32       0.91      0.92      0.92       300\n",
      "          33       0.67      0.86      0.75         7\n",
      "          34       0.82      0.82      0.82       165\n",
      "          35       0.87      0.90      0.88        51\n",
      "          36       0.70      0.90      0.79        31\n",
      "          37       1.00      1.00      1.00        33\n",
      "          38       0.56      0.73      0.63        33\n",
      "          39       0.94      0.97      0.95        30\n",
      "          40       0.54      0.72      0.62        29\n",
      "          41       0.86      0.93      0.89        70\n",
      "          42       0.40      0.67      0.50         6\n",
      "          43       0.64      0.75      0.70        65\n",
      "          44       0.64      0.64      0.64        11\n",
      "          45       0.50      0.29      0.36         7\n",
      "          46       0.85      0.95      0.90        91\n",
      "          47       0.68      0.78      0.72        32\n",
      "          48       0.76      0.75      0.75        63\n",
      "          49       0.43      0.33      0.38         9\n",
      "          50       0.50      0.33      0.40         6\n",
      "          51       0.48      0.62      0.54        16\n",
      "          52       0.71      0.82      0.76        76\n",
      "          53       0.86      0.92      0.89        13\n",
      "          54       0.86      0.82      0.84       167\n",
      "          55       0.50      0.42      0.46        33\n",
      "          56       1.00      0.97      0.98        30\n",
      "          57       0.82      0.88      0.85        42\n",
      "          58       0.79      0.92      0.85        12\n",
      "          59       0.64      0.90      0.75        10\n",
      "          60       0.77      0.68      0.72       190\n",
      "          61       0.81      0.95      0.88        37\n",
      "          62       1.00      1.00      1.00        11\n",
      "          63       0.83      0.79      0.81        19\n",
      "          64       0.67      0.67      0.67        18\n",
      "          65       0.64      0.70      0.67        10\n",
      "          66       0.82      1.00      0.90         9\n",
      "          67       0.90      0.93      0.91        81\n",
      "          68       0.89      0.89      0.89         9\n",
      "          69       0.12      0.33      0.18         3\n",
      "          71       0.33      1.00      0.50         3\n",
      "          72       0.71      0.75      0.73        20\n",
      "          73       0.76      0.90      0.83        21\n",
      "          74       0.46      0.67      0.55         9\n",
      "          75       0.00      0.00      0.00         0\n",
      "          76       0.33      0.50      0.40         2\n",
      "          77       0.50      1.00      0.67         1\n",
      "          78       0.00      0.00      0.00         0\n",
      "          79       0.14      0.17      0.15         6\n",
      "          80       0.75      0.50      0.60         6\n",
      "          82       0.00      0.00      0.00         0\n",
      "          83       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.81      4492\n",
      "   macro avg       0.69      0.75      0.71      4492\n",
      "weighted avg       0.82      0.81      0.81      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"/Users/mac/Desktop/2022春课件/时序/train_semantic2.txt\",lr=0.2, epoch=1000, wordNgrams=3, dim=800,loss='softmax')\n",
    "Y_pre = []\n",
    "for i in range(len(X_test)):\n",
    "    r = model.predict(\" \".join(X_test[i]),k=2)\n",
    "    Y_pre.append(get_label(r))\n",
    "\n",
    "print('准确率', accuracy_score(Y_test, Y_pre))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_pre, average='weighted'))\n",
    "print(classification_report(Y_pre,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b30b29",
   "metadata": {},
   "source": [
    "#### 三级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c65dd50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['三级分类_整数']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train = X_train.reset_index()['segment']\n",
    "Y_train = Y_train.reset_index()['三级分类_整数']\n",
    "X_test = X_test.reset_index()['segment']\n",
    "Y_test = Y_test.reset_index()['三级分类_整数']\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/train_semantic3.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_train)):\n",
    "        str1 = \" \".join(X_train[i])+\"\\t\"+\"__label__\"+str(Y_train[i])+'\\n'\n",
    "        f.write(str1)\n",
    "\n",
    "with open('/Users/mac/Desktop/2022春课件/时序/test_semantic3.txt','w',encoding='utf-8') as f:\n",
    "    for i in range(len(X_test)):\n",
    "        str1 = \" \".join(X_test[i])+\"\\t\"+\"__label__\"+str(Y_test[i])+'\\n'\n",
    "        f.write(str1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "171cffed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.7713713268032057\n",
      "平均f1-score: 0.7779240262969609\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        54\n",
      "           1       0.84      0.76      0.79        82\n",
      "           2       0.75      0.90      0.82        20\n",
      "           3       0.78      0.90      0.84        20\n",
      "           4       0.92      0.90      0.91       163\n",
      "           5       0.92      0.89      0.90        87\n",
      "           6       0.76      0.79      0.77       128\n",
      "           7       0.81      0.74      0.77        34\n",
      "           8       0.42      0.67      0.51        15\n",
      "           9       0.90      0.85      0.87       112\n",
      "          10       0.88      0.93      0.90        30\n",
      "          11       0.78      0.78      0.78       126\n",
      "          12       0.74      0.81      0.77        31\n",
      "          13       0.82      0.88      0.85        48\n",
      "          14       0.75      0.75      0.75         4\n",
      "          15       0.57      0.74      0.65        27\n",
      "          16       0.92      0.90      0.91       181\n",
      "          17       0.75      0.87      0.81        95\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.83      0.96      0.89        25\n",
      "          20       0.80      0.90      0.85        70\n",
      "          21       0.63      0.64      0.64        78\n",
      "          22       0.67      0.44      0.53       142\n",
      "          23       0.68      0.72      0.70       136\n",
      "          24       0.75      0.91      0.82        46\n",
      "          25       0.86      0.90      0.88       131\n",
      "          26       0.62      0.53      0.57        34\n",
      "          27       0.62      0.73      0.67        11\n",
      "          28       0.82      0.89      0.86       114\n",
      "          29       0.57      0.68      0.62        59\n",
      "          30       0.50      0.83      0.62         6\n",
      "          31       0.93      0.95      0.94        57\n",
      "          32       0.75      1.00      0.86         3\n",
      "          33       0.88      0.96      0.92        24\n",
      "          34       0.96      0.90      0.93        30\n",
      "          35       0.67      0.50      0.57         8\n",
      "          36       0.92      0.55      0.69        44\n",
      "          37       0.80      0.85      0.83       103\n",
      "          38       0.56      0.83      0.67         6\n",
      "          39       0.89      0.87      0.88       129\n",
      "          40       0.90      0.95      0.92        39\n",
      "          41       0.75      1.00      0.86         3\n",
      "          42       0.50      1.00      0.67         6\n",
      "          43       0.71      1.00      0.83        10\n",
      "          44       0.92      0.77      0.84        30\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       0.59      0.64      0.61        36\n",
      "          47       0.83      0.71      0.77        14\n",
      "          48       0.92      0.92      0.92        12\n",
      "          49       0.94      0.89      0.92        19\n",
      "          50       0.56      0.65      0.60        34\n",
      "          51       0.80      0.91      0.85        22\n",
      "          52       0.30      0.38      0.33         8\n",
      "          53       1.00      0.78      0.88         9\n",
      "          54       0.64      0.75      0.70        65\n",
      "          55       0.33      1.00      0.50         1\n",
      "          56       0.50      0.25      0.33         8\n",
      "          57       0.75      1.00      0.86         3\n",
      "          58       1.00      0.89      0.94        18\n",
      "          59       0.86      1.00      0.92         6\n",
      "          60       0.20      0.25      0.22         4\n",
      "          61       0.64      0.58      0.61        24\n",
      "          62       0.74      0.76      0.75        34\n",
      "          63       0.68      0.76      0.71        33\n",
      "          64       0.73      0.70      0.71        64\n",
      "          65       0.65      0.68      0.67        22\n",
      "          66       0.86      0.90      0.88       103\n",
      "          67       1.00      0.94      0.97        17\n",
      "          68       0.71      0.45      0.56        11\n",
      "          69       0.50      0.40      0.44         5\n",
      "          70       0.57      0.60      0.59        20\n",
      "          71       0.71      0.78      0.74        78\n",
      "          72       0.93      0.76      0.84        17\n",
      "          73       0.71      0.83      0.77         6\n",
      "          74       0.80      0.70      0.75        87\n",
      "          75       0.82      0.82      0.82        28\n",
      "          76       0.46      0.42      0.44        31\n",
      "          77       0.90      1.00      0.95        28\n",
      "          78       0.75      0.75      0.75         8\n",
      "          79       1.00      0.97      0.98        30\n",
      "          80       0.85      1.00      0.92        22\n",
      "          81       0.46      0.60      0.52        10\n",
      "          82       0.79      0.92      0.85        12\n",
      "          83       0.67      0.80      0.73         5\n",
      "          84       0.62      0.83      0.71         6\n",
      "          85       0.53      0.65      0.58        69\n",
      "          86       1.00      1.00      1.00         1\n",
      "          87       0.72      0.89      0.80        53\n",
      "          88       0.82      0.75      0.78        12\n",
      "          89       0.94      0.92      0.93        36\n",
      "          90       0.79      0.97      0.87        35\n",
      "          91       1.00      0.91      0.95        11\n",
      "          92       0.19      0.38      0.25         8\n",
      "          93       0.83      0.75      0.79        20\n",
      "          94       0.72      0.62      0.67        21\n",
      "          95       0.74      0.93      0.83        28\n",
      "          96       0.55      0.50      0.52        12\n",
      "          97       0.91      1.00      0.95        10\n",
      "          98       0.86      0.92      0.89        48\n",
      "          99       0.77      0.54      0.63       146\n",
      "         100       0.82      1.00      0.90         9\n",
      "         101       0.88      0.97      0.92        29\n",
      "         102       0.83      0.06      0.12        80\n",
      "         103       0.65      0.65      0.65        84\n",
      "         104       0.12      0.17      0.14         6\n",
      "         106       0.44      0.80      0.57         5\n",
      "         107       0.90      0.90      0.90        40\n",
      "         108       0.47      0.78      0.58         9\n",
      "         109       0.57      1.00      0.73         4\n",
      "         110       0.67      0.74      0.70        19\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.88      0.81      0.85        27\n",
      "         113       0.33      1.00      0.50         1\n",
      "         114       0.00      0.00      0.00         2\n",
      "         115       0.50      0.62      0.56         8\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.33      0.20      0.25         5\n",
      "         118       0.50      0.33      0.40         3\n",
      "         119       0.55      0.25      0.34        24\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.79      0.68      0.73        22\n",
      "         122       0.29      0.17      0.21        12\n",
      "         123       0.00      0.00      0.00         1\n",
      "         124       0.67      1.00      0.80         4\n",
      "         125       0.75      0.43      0.55         7\n",
      "         126       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "         130       0.00      0.00      0.00         0\n",
      "         131       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.77      4492\n",
      "   macro avg       0.68      0.71      0.68      4492\n",
      "weighted avg       0.78      0.77      0.76      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=\"/Users/mac/Desktop/2022春课件/时序/train_semantic3.txt\",lr=0.1, epoch=1000, wordNgrams=3, dim=1000,loss='softmax')\n",
    "Y_pre = []\n",
    "for i in range(len(X_test)):\n",
    "    r = model.predict(\" \".join(X_test[i]),k=2)\n",
    "    Y_pre.append(get_label(r))\n",
    "\n",
    "print('准确率', accuracy_score(Y_test, Y_pre))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_pre, average='weighted'))\n",
    "print(classification_report(Y_pre,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e751e4",
   "metadata": {},
   "source": [
    "## textcnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ee6c53",
   "metadata": {},
   "source": [
    "#### 格式处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "00f86221",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data['segment'])\n",
    "vocab=tokenizer.word_index\n",
    "X = data['segment']\n",
    "maxlen = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33964d26",
   "metadata": {},
   "source": [
    "### 简单CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746d4987",
   "metadata": {},
   "source": [
    "#### 一级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d51440cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['一级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ba7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 100)      1288200     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 30, 32)       6432        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 30, 32)       9632        embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 30, 32)       12832       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 32)        0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 32)        0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 32)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 96)        0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 96)           0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 96)           0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 23)           2231        dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,319,327\n",
      "Trainable params: 1,319,327\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 100, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(32, 2, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(32, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(32, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_1 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "625b284f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/20\n",
      "8383/8383 [==============================] - 6s 742us/sample - loss: 2.6980 - accuracy: 0.2806 - val_loss: 2.3535 - val_accuracy: 0.3101\n",
      "Epoch 2/20\n",
      "8383/8383 [==============================] - 5s 537us/sample - loss: 2.1731 - accuracy: 0.3638 - val_loss: 1.9565 - val_accuracy: 0.4699\n",
      "Epoch 3/20\n",
      "8383/8383 [==============================] - 5s 630us/sample - loss: 1.6048 - accuracy: 0.5852 - val_loss: 1.3677 - val_accuracy: 0.6789\n",
      "Epoch 4/20\n",
      "8383/8383 [==============================] - 5s 604us/sample - loss: 1.0016 - accuracy: 0.7803 - val_loss: 0.9636 - val_accuracy: 0.7791\n",
      "Epoch 5/20\n",
      "8383/8383 [==============================] - 4s 528us/sample - loss: 0.6309 - accuracy: 0.8674 - val_loss: 0.7426 - val_accuracy: 0.8244\n",
      "Epoch 6/20\n",
      "8383/8383 [==============================] - 4s 519us/sample - loss: 0.4146 - accuracy: 0.9166 - val_loss: 0.6253 - val_accuracy: 0.8583\n",
      "Epoch 7/20\n",
      "8383/8383 [==============================] - 4s 530us/sample - loss: 0.2891 - accuracy: 0.9450 - val_loss: 0.5584 - val_accuracy: 0.8698\n",
      "Epoch 8/20\n",
      "8383/8383 [==============================] - 4s 515us/sample - loss: 0.2111 - accuracy: 0.9616 - val_loss: 0.5179 - val_accuracy: 0.8750\n",
      "Epoch 9/20\n",
      "8383/8383 [==============================] - 4s 517us/sample - loss: 0.1616 - accuracy: 0.9698 - val_loss: 0.4978 - val_accuracy: 0.8831\n",
      "Epoch 10/20\n",
      "8383/8383 [==============================] - 4s 513us/sample - loss: 0.1275 - accuracy: 0.9744 - val_loss: 0.4843 - val_accuracy: 0.8845\n",
      "Epoch 11/20\n",
      "8383/8383 [==============================] - 4s 518us/sample - loss: 0.1029 - accuracy: 0.9797 - val_loss: 0.4710 - val_accuracy: 0.8841\n",
      "Epoch 12/20\n",
      "8383/8383 [==============================] - 4s 529us/sample - loss: 0.0859 - accuracy: 0.9832 - val_loss: 0.4655 - val_accuracy: 0.8822\n",
      "Epoch 13/20\n",
      "8383/8383 [==============================] - 4s 521us/sample - loss: 0.0741 - accuracy: 0.9850 - val_loss: 0.4613 - val_accuracy: 0.8850\n",
      "Epoch 14/20\n",
      "8383/8383 [==============================] - 5s 543us/sample - loss: 0.0652 - accuracy: 0.9860 - val_loss: 0.4624 - val_accuracy: 0.8836\n",
      "Epoch 15/20\n",
      "8383/8383 [==============================] - 4s 524us/sample - loss: 0.0558 - accuracy: 0.9882 - val_loss: 0.4613 - val_accuracy: 0.8826\n",
      "Epoch 16/20\n",
      "8383/8383 [==============================] - 4s 515us/sample - loss: 0.0471 - accuracy: 0.9906 - val_loss: 0.4580 - val_accuracy: 0.8855\n",
      "Epoch 17/20\n",
      "8383/8383 [==============================] - 4s 528us/sample - loss: 0.0433 - accuracy: 0.9912 - val_loss: 0.4614 - val_accuracy: 0.8841\n",
      "Epoch 18/20\n",
      "8383/8383 [==============================] - 4s 526us/sample - loss: 0.0389 - accuracy: 0.9916 - val_loss: 0.4603 - val_accuracy: 0.8865\n",
      "Epoch 19/20\n",
      "8383/8383 [==============================] - 5s 546us/sample - loss: 0.0358 - accuracy: 0.9920 - val_loss: 0.4634 - val_accuracy: 0.8860\n",
      "Epoch 20/20\n",
      "8383/8383 [==============================] - 5s 544us/sample - loss: 0.0332 - accuracy: 0.9928 - val_loss: 0.4675 - val_accuracy: 0.8850\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15a5b1810>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_1.fit(X_train_padded_seqs, one_hot_labels,  epochs=20, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa7eab4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.8873552983081033\n",
      "平均f1-score: 0.8850686674618192\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.96      0.94       128\n",
      "           1       0.92      0.91      0.91       469\n",
      "           2       0.96      0.95      0.96       199\n",
      "           3       0.95      0.90      0.92      1442\n",
      "           4       0.89      0.90      0.89       217\n",
      "           5       0.93      0.93      0.93       338\n",
      "           6       0.89      0.89      0.89       450\n",
      "           7       0.76      0.80      0.78       112\n",
      "           8       0.84      0.89      0.87       152\n",
      "           9       0.85      0.90      0.87       358\n",
      "          10       0.60      0.77      0.67        60\n",
      "          11       0.82      0.72      0.77       246\n",
      "          12       0.79      0.83      0.81        54\n",
      "          13       0.86      1.00      0.92        30\n",
      "          14       0.63      0.67      0.65        51\n",
      "          15       0.70      0.89      0.78        35\n",
      "          16       0.10      0.14      0.12         7\n",
      "          17       0.88      0.95      0.91       118\n",
      "          18       0.68      0.85      0.76        20\n",
      "          19       0.36      0.80      0.50         5\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89      4492\n",
      "   macro avg       0.67      0.72      0.69      4492\n",
      "weighted avg       0.89      0.89      0.89      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_1.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa69cf7",
   "metadata": {},
   "source": [
    "#### 二级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ab8c3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['二级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "10f77f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 300)      3864600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 30, 64)       38464       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 30, 64)       57664       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 30, 64)       76864       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 64)        0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 64)        0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 64)        0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 192)       0           max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 192)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 192)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 84)           16212       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,053,804\n",
      "Trainable params: 4,053,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(64, 2, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(64, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(64, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_2 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b868b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/50\n",
      "8383/8383 [==============================] - 15s 2ms/sample - loss: 4.0217 - accuracy: 0.0910 - val_loss: 3.7024 - val_accuracy: 0.1684\n",
      "Epoch 2/50\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 3.3107 - accuracy: 0.2482 - val_loss: 2.9593 - val_accuracy: 0.3683\n",
      "Epoch 3/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 2.3163 - accuracy: 0.5653 - val_loss: 2.0409 - val_accuracy: 0.6102\n",
      "Epoch 4/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 1.3697 - accuracy: 0.7613 - val_loss: 1.4638 - val_accuracy: 0.7052\n",
      "Epoch 5/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.8157 - accuracy: 0.8566 - val_loss: 1.1746 - val_accuracy: 0.7595\n",
      "Epoch 6/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.4975 - accuracy: 0.9142 - val_loss: 1.0259 - val_accuracy: 0.7948\n",
      "Epoch 7/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.3258 - accuracy: 0.9410 - val_loss: 0.9474 - val_accuracy: 0.8053\n",
      "Epoch 8/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.2209 - accuracy: 0.9608 - val_loss: 0.8940 - val_accuracy: 0.8101\n",
      "Epoch 9/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.1636 - accuracy: 0.9691 - val_loss: 0.8676 - val_accuracy: 0.8173\n",
      "Epoch 10/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.1250 - accuracy: 0.9765 - val_loss: 0.8505 - val_accuracy: 0.8173\n",
      "Epoch 11/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0995 - accuracy: 0.9801 - val_loss: 0.8377 - val_accuracy: 0.8206\n",
      "Epoch 12/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0804 - accuracy: 0.9846 - val_loss: 0.8369 - val_accuracy: 0.8197\n",
      "Epoch 13/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0692 - accuracy: 0.9865 - val_loss: 0.8358 - val_accuracy: 0.8206\n",
      "Epoch 14/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0582 - accuracy: 0.9863 - val_loss: 0.8371 - val_accuracy: 0.8197\n",
      "Epoch 15/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0551 - accuracy: 0.9880 - val_loss: 0.8389 - val_accuracy: 0.8211\n",
      "Epoch 16/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0463 - accuracy: 0.9895 - val_loss: 0.8447 - val_accuracy: 0.8197\n",
      "Epoch 17/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0452 - accuracy: 0.9890 - val_loss: 0.8501 - val_accuracy: 0.8192\n",
      "Epoch 18/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0359 - accuracy: 0.9921 - val_loss: 0.8596 - val_accuracy: 0.8158\n",
      "Epoch 19/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0336 - accuracy: 0.9908 - val_loss: 0.8541 - val_accuracy: 0.8206\n",
      "Epoch 20/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0327 - accuracy: 0.9918 - val_loss: 0.8636 - val_accuracy: 0.8206\n",
      "Epoch 21/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0281 - accuracy: 0.9926 - val_loss: 0.8630 - val_accuracy: 0.8168\n",
      "Epoch 22/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0292 - accuracy: 0.9919 - val_loss: 0.8689 - val_accuracy: 0.8192\n",
      "Epoch 23/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0281 - accuracy: 0.9907 - val_loss: 0.8700 - val_accuracy: 0.8187\n",
      "Epoch 24/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0263 - accuracy: 0.9922 - val_loss: 0.8699 - val_accuracy: 0.8201\n",
      "Epoch 25/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0247 - accuracy: 0.9932 - val_loss: 0.8838 - val_accuracy: 0.8182\n",
      "Epoch 26/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0250 - accuracy: 0.9926 - val_loss: 0.8822 - val_accuracy: 0.8173\n",
      "Epoch 27/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0232 - accuracy: 0.9928 - val_loss: 0.8839 - val_accuracy: 0.8197\n",
      "Epoch 28/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0213 - accuracy: 0.9924 - val_loss: 0.8918 - val_accuracy: 0.8187\n",
      "Epoch 29/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0190 - accuracy: 0.9945 - val_loss: 0.8907 - val_accuracy: 0.8173\n",
      "Epoch 30/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0199 - accuracy: 0.9927 - val_loss: 0.9007 - val_accuracy: 0.8158\n",
      "Epoch 31/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0193 - accuracy: 0.9938 - val_loss: 0.9027 - val_accuracy: 0.8192\n",
      "Epoch 32/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0190 - accuracy: 0.9940 - val_loss: 0.9062 - val_accuracy: 0.8173\n",
      "Epoch 33/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0201 - accuracy: 0.9937 - val_loss: 0.9108 - val_accuracy: 0.8177\n",
      "Epoch 34/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0181 - accuracy: 0.9942 - val_loss: 0.9164 - val_accuracy: 0.8192\n",
      "Epoch 35/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0196 - accuracy: 0.9928 - val_loss: 0.9240 - val_accuracy: 0.8192\n",
      "Epoch 36/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0209 - accuracy: 0.9921 - val_loss: 0.9247 - val_accuracy: 0.8182\n",
      "Epoch 37/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0195 - accuracy: 0.9930 - val_loss: 0.9299 - val_accuracy: 0.8187\n",
      "Epoch 38/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0166 - accuracy: 0.9939 - val_loss: 0.9376 - val_accuracy: 0.8192\n",
      "Epoch 39/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0154 - accuracy: 0.9949 - val_loss: 0.9390 - val_accuracy: 0.8177\n",
      "Epoch 40/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0176 - accuracy: 0.9928 - val_loss: 0.9395 - val_accuracy: 0.8168\n",
      "Epoch 41/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0153 - accuracy: 0.9945 - val_loss: 0.9421 - val_accuracy: 0.8154\n",
      "Epoch 42/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0168 - accuracy: 0.9936 - val_loss: 0.9485 - val_accuracy: 0.8163\n",
      "Epoch 43/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0168 - accuracy: 0.9942 - val_loss: 0.9512 - val_accuracy: 0.8168\n",
      "Epoch 44/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0155 - accuracy: 0.9938 - val_loss: 0.9515 - val_accuracy: 0.8149\n",
      "Epoch 45/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0152 - accuracy: 0.9943 - val_loss: 0.9506 - val_accuracy: 0.8173\n",
      "Epoch 46/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0149 - accuracy: 0.9938 - val_loss: 0.9551 - val_accuracy: 0.8187\n",
      "Epoch 47/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0147 - accuracy: 0.9944 - val_loss: 0.9584 - val_accuracy: 0.8177\n",
      "Epoch 48/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0151 - accuracy: 0.9937 - val_loss: 0.9676 - val_accuracy: 0.8182\n",
      "Epoch 49/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0150 - accuracy: 0.9938 - val_loss: 0.9708 - val_accuracy: 0.8163\n",
      "Epoch 50/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0151 - accuracy: 0.9942 - val_loss: 0.9749 - val_accuracy: 0.8182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16b3f28d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_2.fit(X_train_padded_seqs, one_hot_labels,  epochs=50, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1278c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.81233303650935\n",
      "平均f1-score: 0.8113607490231142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        57\n",
      "           1       0.85      0.79      0.82        85\n",
      "           2       0.97      0.90      0.93       197\n",
      "           3       0.90      0.94      0.92        49\n",
      "           4       0.90      0.84      0.87       134\n",
      "           5       0.77      0.78      0.78       130\n",
      "           6       0.85      0.97      0.90        58\n",
      "           7       0.90      0.82      0.86       116\n",
      "           8       0.94      0.84      0.89        57\n",
      "           9       0.91      0.89      0.90       223\n",
      "          10       0.88      0.91      0.89        54\n",
      "          11       0.84      0.83      0.83        52\n",
      "          12       0.74      0.90      0.81        48\n",
      "          13       0.64      0.65      0.64        60\n",
      "          14       0.89      0.90      0.90       174\n",
      "          15       0.82      0.88      0.85       103\n",
      "          16       1.00      1.00      1.00        12\n",
      "          17       0.83      0.92      0.87        26\n",
      "          18       0.71      0.79      0.75        71\n",
      "          19       0.67      0.44      0.53       142\n",
      "          20       0.63      0.67      0.65       137\n",
      "          21       0.76      0.92      0.83        51\n",
      "          22       0.80      0.84      0.82       131\n",
      "          23       0.55      0.64      0.59        25\n",
      "          24       0.69      0.69      0.69        13\n",
      "          25       0.81      0.93      0.87       107\n",
      "          26       0.67      0.70      0.69        67\n",
      "          27       0.20      1.00      0.33         2\n",
      "          28       0.95      0.97      0.96        61\n",
      "          29       0.81      0.95      0.88        22\n",
      "          30       0.93      0.90      0.91        29\n",
      "          31       0.50      1.00      0.67         3\n",
      "          32       0.94      0.85      0.89       336\n",
      "          33       0.67      0.75      0.71         8\n",
      "          34       0.85      0.82      0.84       172\n",
      "          35       0.92      0.92      0.92        53\n",
      "          36       0.78      0.79      0.78        39\n",
      "          37       0.97      1.00      0.98        32\n",
      "          38       0.60      0.81      0.69        32\n",
      "          39       0.87      0.84      0.86        32\n",
      "          40       0.54      0.70      0.61        30\n",
      "          41       0.84      0.91      0.88        70\n",
      "          42       0.20      0.33      0.25         6\n",
      "          43       0.67      0.61      0.64        83\n",
      "          44       0.64      0.78      0.70         9\n",
      "          45       0.75      0.33      0.46         9\n",
      "          46       0.89      0.89      0.89       101\n",
      "          47       0.65      0.83      0.73        29\n",
      "          48       0.68      0.79      0.73        53\n",
      "          49       0.43      0.38      0.40         8\n",
      "          50       0.25      0.33      0.29         3\n",
      "          51       0.67      0.56      0.61        25\n",
      "          52       0.67      0.67      0.67        87\n",
      "          53       0.79      0.79      0.79        14\n",
      "          54       0.81      0.82      0.81       157\n",
      "          55       0.54      0.68      0.60        22\n",
      "          56       0.93      1.00      0.96        27\n",
      "          57       0.80      0.95      0.87        38\n",
      "          58       0.93      0.87      0.90        15\n",
      "          59       0.71      0.91      0.80        11\n",
      "          60       0.87      0.66      0.75       222\n",
      "          61       0.86      0.93      0.89        40\n",
      "          62       0.91      1.00      0.95        10\n",
      "          63       0.78      0.64      0.70        22\n",
      "          64       0.56      0.91      0.69        11\n",
      "          65       0.45      0.62      0.53         8\n",
      "          66       0.82      0.90      0.86        10\n",
      "          67       0.92      0.90      0.91        84\n",
      "          68       1.00      0.82      0.90        11\n",
      "          69       0.12      0.50      0.20         2\n",
      "          71       0.22      1.00      0.36         2\n",
      "          72       0.71      0.75      0.73        20\n",
      "          73       0.76      0.79      0.78        24\n",
      "          74       0.69      0.75      0.72        12\n",
      "          75       0.00      0.00      0.00         0\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.50      1.00      0.67         1\n",
      "          78       0.00      0.00      0.00         0\n",
      "          79       0.29      0.29      0.29         7\n",
      "          80       0.25      0.14      0.18         7\n",
      "          82       0.00      0.00      0.00         0\n",
      "          83       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.81      4492\n",
      "   macro avg       0.68      0.74      0.70      4492\n",
      "weighted avg       0.82      0.81      0.81      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_2.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e7b96",
   "metadata": {},
   "source": [
    "#### 三级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a55f27ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['三级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "968ec632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 30, 300)      3864600     input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_6 (Conv1D)               (None, 30, 128)      76928       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_7 (Conv1D)               (None, 30, 128)      115328      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_8 (Conv1D)               (None, 30, 128)      153728      embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_6 (MaxPooling1D)  (None, 1, 128)       0           conv1d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_7 (MaxPooling1D)  (None, 1, 128)       0           conv1d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_8 (MaxPooling1D)  (None, 1, 128)       0           conv1d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 1, 384)       0           max_pooling1d_6[0][0]            \n",
      "                                                                 max_pooling1d_7[0][0]            \n",
      "                                                                 max_pooling1d_8[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 384)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 384)          0           flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 133)          51205       dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,261,789\n",
      "Trainable params: 4,261,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(128, 2, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(128, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(128, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_3 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "831d3715",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 4.4052 - accuracy: 0.0860 - val_loss: 4.0772 - val_accuracy: 0.1641\n",
      "Epoch 2/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 3.6456 - accuracy: 0.2620 - val_loss: 3.2290 - val_accuracy: 0.3764\n",
      "Epoch 3/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 2.4951 - accuracy: 0.5379 - val_loss: 2.2405 - val_accuracy: 0.5797\n",
      "Epoch 4/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 1.4952 - accuracy: 0.7364 - val_loss: 1.6592 - val_accuracy: 0.6837\n",
      "Epoch 5/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.8855 - accuracy: 0.8423 - val_loss: 1.3571 - val_accuracy: 0.7309\n",
      "Epoch 6/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.5346 - accuracy: 0.9041 - val_loss: 1.1903 - val_accuracy: 0.7514\n",
      "Epoch 7/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.3369 - accuracy: 0.9362 - val_loss: 1.1084 - val_accuracy: 0.7643\n",
      "Epoch 8/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.2202 - accuracy: 0.9566 - val_loss: 1.0577 - val_accuracy: 0.7715\n",
      "Epoch 9/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.1614 - accuracy: 0.9656 - val_loss: 1.0294 - val_accuracy: 0.7762\n",
      "Epoch 10/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.1176 - accuracy: 0.9746 - val_loss: 1.0198 - val_accuracy: 0.7801\n",
      "Epoch 11/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0923 - accuracy: 0.9786 - val_loss: 1.0213 - val_accuracy: 0.7753\n",
      "Epoch 12/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0766 - accuracy: 0.9833 - val_loss: 1.0149 - val_accuracy: 0.7801\n",
      "Epoch 13/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0612 - accuracy: 0.9853 - val_loss: 1.0228 - val_accuracy: 0.7796\n",
      "Epoch 14/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0557 - accuracy: 0.9868 - val_loss: 1.0215 - val_accuracy: 0.7781\n",
      "Epoch 15/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0484 - accuracy: 0.9871 - val_loss: 1.0267 - val_accuracy: 0.7810\n",
      "Epoch 16/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0429 - accuracy: 0.9894 - val_loss: 1.0287 - val_accuracy: 0.7815\n",
      "Epoch 17/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0397 - accuracy: 0.9885 - val_loss: 1.0326 - val_accuracy: 0.7810\n",
      "Epoch 18/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0370 - accuracy: 0.9894 - val_loss: 1.0419 - val_accuracy: 0.7801\n",
      "Epoch 19/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0344 - accuracy: 0.9890 - val_loss: 1.0554 - val_accuracy: 0.7786\n",
      "Epoch 20/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0322 - accuracy: 0.9900 - val_loss: 1.0537 - val_accuracy: 0.7786\n",
      "Epoch 21/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0306 - accuracy: 0.9896 - val_loss: 1.0611 - val_accuracy: 0.7805\n",
      "Epoch 22/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0279 - accuracy: 0.9921 - val_loss: 1.0636 - val_accuracy: 0.7805\n",
      "Epoch 23/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0306 - accuracy: 0.9901 - val_loss: 1.0686 - val_accuracy: 0.7820\n",
      "Epoch 24/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0277 - accuracy: 0.9916 - val_loss: 1.0743 - val_accuracy: 0.7820\n",
      "Epoch 25/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0270 - accuracy: 0.9916 - val_loss: 1.0801 - val_accuracy: 0.7796\n",
      "Epoch 26/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0242 - accuracy: 0.9911 - val_loss: 1.0840 - val_accuracy: 0.7801\n",
      "Epoch 27/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0257 - accuracy: 0.9909 - val_loss: 1.0871 - val_accuracy: 0.7791\n",
      "Epoch 28/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0240 - accuracy: 0.9906 - val_loss: 1.0952 - val_accuracy: 0.7781\n",
      "Epoch 29/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0251 - accuracy: 0.9920 - val_loss: 1.1059 - val_accuracy: 0.7777\n",
      "Epoch 30/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0227 - accuracy: 0.9907 - val_loss: 1.1027 - val_accuracy: 0.7801\n",
      "Epoch 31/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0237 - accuracy: 0.9914 - val_loss: 1.1054 - val_accuracy: 0.7758\n",
      "Epoch 32/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0221 - accuracy: 0.9919 - val_loss: 1.1186 - val_accuracy: 0.7772\n",
      "Epoch 33/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0224 - accuracy: 0.9918 - val_loss: 1.1220 - val_accuracy: 0.7734\n",
      "Epoch 34/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0222 - accuracy: 0.9912 - val_loss: 1.1222 - val_accuracy: 0.7767\n",
      "Epoch 35/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0204 - accuracy: 0.9921 - val_loss: 1.1230 - val_accuracy: 0.7767\n",
      "Epoch 36/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0215 - accuracy: 0.9925 - val_loss: 1.1260 - val_accuracy: 0.7786\n",
      "Epoch 37/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0197 - accuracy: 0.9914 - val_loss: 1.1309 - val_accuracy: 0.7767\n",
      "Epoch 38/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0204 - accuracy: 0.9916 - val_loss: 1.1418 - val_accuracy: 0.7777\n",
      "Epoch 39/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0195 - accuracy: 0.9924 - val_loss: 1.1340 - val_accuracy: 0.7762\n",
      "Epoch 40/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0202 - accuracy: 0.9906 - val_loss: 1.1441 - val_accuracy: 0.7786\n",
      "Epoch 41/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0199 - accuracy: 0.9924 - val_loss: 1.1419 - val_accuracy: 0.7815\n",
      "Epoch 42/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0191 - accuracy: 0.9916 - val_loss: 1.1523 - val_accuracy: 0.7791\n",
      "Epoch 43/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0202 - accuracy: 0.9911 - val_loss: 1.1498 - val_accuracy: 0.7781\n",
      "Epoch 44/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0194 - accuracy: 0.9920 - val_loss: 1.1490 - val_accuracy: 0.7767\n",
      "Epoch 45/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0175 - accuracy: 0.9922 - val_loss: 1.1527 - val_accuracy: 0.7791\n",
      "Epoch 46/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0189 - accuracy: 0.9919 - val_loss: 1.1604 - val_accuracy: 0.7786\n",
      "Epoch 47/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0186 - accuracy: 0.9920 - val_loss: 1.1579 - val_accuracy: 0.7801\n",
      "Epoch 48/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0180 - accuracy: 0.9918 - val_loss: 1.1566 - val_accuracy: 0.7777\n",
      "Epoch 49/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0170 - accuracy: 0.9926 - val_loss: 1.1646 - val_accuracy: 0.7801\n",
      "Epoch 50/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0192 - accuracy: 0.9920 - val_loss: 1.1739 - val_accuracy: 0.7772\n",
      "Epoch 51/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0175 - accuracy: 0.9922 - val_loss: 1.1762 - val_accuracy: 0.7743\n",
      "Epoch 52/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0171 - accuracy: 0.9925 - val_loss: 1.1698 - val_accuracy: 0.7772\n",
      "Epoch 53/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0165 - accuracy: 0.9921 - val_loss: 1.1786 - val_accuracy: 0.7767\n",
      "Epoch 54/100\n",
      "8383/8383 [==============================] - 20s 2ms/sample - loss: 0.0163 - accuracy: 0.9924 - val_loss: 1.1858 - val_accuracy: 0.7777\n",
      "Epoch 55/100\n",
      "8383/8383 [==============================] - 21s 2ms/sample - loss: 0.0163 - accuracy: 0.9921 - val_loss: 1.1881 - val_accuracy: 0.7767\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "8383/8383 [==============================] - 180s 21ms/sample - loss: 0.0177 - accuracy: 0.9918 - val_loss: 1.1946 - val_accuracy: 0.7762\n",
      "Epoch 57/100\n",
      "8383/8383 [==============================] - 78s 9ms/sample - loss: 0.0185 - accuracy: 0.9918 - val_loss: 1.1919 - val_accuracy: 0.7767\n",
      "Epoch 58/100\n",
      "8383/8383 [==============================] - 32s 4ms/sample - loss: 0.0162 - accuracy: 0.9920 - val_loss: 1.2005 - val_accuracy: 0.7758\n",
      "Epoch 59/100\n",
      "8383/8383 [==============================] - 2529s 302ms/sample - loss: 0.0164 - accuracy: 0.9925 - val_loss: 1.1969 - val_accuracy: 0.7758\n",
      "Epoch 60/100\n",
      "8383/8383 [==============================] - 21s 2ms/sample - loss: 0.0169 - accuracy: 0.9919 - val_loss: 1.2040 - val_accuracy: 0.7748\n",
      "Epoch 61/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0147 - accuracy: 0.9922 - val_loss: 1.2070 - val_accuracy: 0.7715\n",
      "Epoch 62/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0168 - accuracy: 0.9927 - val_loss: 1.2065 - val_accuracy: 0.7753\n",
      "Epoch 63/100\n",
      "8383/8383 [==============================] - 18s 2ms/sample - loss: 0.0177 - accuracy: 0.9924 - val_loss: 1.2036 - val_accuracy: 0.7758\n",
      "Epoch 64/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0153 - accuracy: 0.9924 - val_loss: 1.2095 - val_accuracy: 0.7758\n",
      "Epoch 65/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0172 - accuracy: 0.9915 - val_loss: 1.2168 - val_accuracy: 0.7748\n",
      "Epoch 66/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0159 - accuracy: 0.9921 - val_loss: 1.2176 - val_accuracy: 0.7758\n",
      "Epoch 67/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0160 - accuracy: 0.9918 - val_loss: 1.2321 - val_accuracy: 0.7753\n",
      "Epoch 68/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0167 - accuracy: 0.9922 - val_loss: 1.2319 - val_accuracy: 0.7734\n",
      "Epoch 69/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0154 - accuracy: 0.9920 - val_loss: 1.2324 - val_accuracy: 0.7767\n",
      "Epoch 70/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0157 - accuracy: 0.9921 - val_loss: 1.2330 - val_accuracy: 0.7772\n",
      "Epoch 71/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0148 - accuracy: 0.9919 - val_loss: 1.2375 - val_accuracy: 0.7729\n",
      "Epoch 72/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0153 - accuracy: 0.9922 - val_loss: 1.2458 - val_accuracy: 0.7762\n",
      "Epoch 73/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0163 - accuracy: 0.9915 - val_loss: 1.2376 - val_accuracy: 0.7791\n",
      "Epoch 74/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0162 - accuracy: 0.9918 - val_loss: 1.2425 - val_accuracy: 0.7767\n",
      "Epoch 75/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0145 - accuracy: 0.9932 - val_loss: 1.2430 - val_accuracy: 0.7772\n",
      "Epoch 76/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0152 - accuracy: 0.9925 - val_loss: 1.2509 - val_accuracy: 0.7786\n",
      "Epoch 77/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0137 - accuracy: 0.9934 - val_loss: 1.2488 - val_accuracy: 0.7786\n",
      "Epoch 78/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0150 - accuracy: 0.9930 - val_loss: 1.2556 - val_accuracy: 0.7777\n",
      "Epoch 79/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0143 - accuracy: 0.9932 - val_loss: 1.2556 - val_accuracy: 0.7734\n",
      "Epoch 80/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0153 - accuracy: 0.9921 - val_loss: 1.2564 - val_accuracy: 0.7767\n",
      "Epoch 81/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0141 - accuracy: 0.9926 - val_loss: 1.2633 - val_accuracy: 0.7781\n",
      "Epoch 82/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0149 - accuracy: 0.9925 - val_loss: 1.2699 - val_accuracy: 0.7786\n",
      "Epoch 83/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0156 - accuracy: 0.9925 - val_loss: 1.2726 - val_accuracy: 0.7777\n",
      "Epoch 84/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0148 - accuracy: 0.9922 - val_loss: 1.2825 - val_accuracy: 0.7753\n",
      "Epoch 85/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0146 - accuracy: 0.9928 - val_loss: 1.2795 - val_accuracy: 0.7767\n",
      "Epoch 86/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0125 - accuracy: 0.9926 - val_loss: 1.2828 - val_accuracy: 0.7753\n",
      "Epoch 87/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0141 - accuracy: 0.9927 - val_loss: 1.2819 - val_accuracy: 0.7767\n",
      "Epoch 88/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0154 - accuracy: 0.9918 - val_loss: 1.2877 - val_accuracy: 0.7772\n",
      "Epoch 89/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0132 - accuracy: 0.9928 - val_loss: 1.2824 - val_accuracy: 0.7772\n",
      "Epoch 90/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0154 - accuracy: 0.9914 - val_loss: 1.2913 - val_accuracy: 0.7791\n",
      "Epoch 91/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0142 - accuracy: 0.9922 - val_loss: 1.2951 - val_accuracy: 0.7801\n",
      "Epoch 92/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0138 - accuracy: 0.9933 - val_loss: 1.2952 - val_accuracy: 0.7767\n",
      "Epoch 93/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0156 - accuracy: 0.9913 - val_loss: 1.3077 - val_accuracy: 0.7767\n",
      "Epoch 94/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0134 - accuracy: 0.9920 - val_loss: 1.2932 - val_accuracy: 0.7781\n",
      "Epoch 95/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0131 - accuracy: 0.9930 - val_loss: 1.3060 - val_accuracy: 0.7753\n",
      "Epoch 96/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0146 - accuracy: 0.9914 - val_loss: 1.3128 - val_accuracy: 0.7762\n",
      "Epoch 97/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0146 - accuracy: 0.9919 - val_loss: 1.3113 - val_accuracy: 0.7772\n",
      "Epoch 98/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0146 - accuracy: 0.9919 - val_loss: 1.3101 - val_accuracy: 0.7748\n",
      "Epoch 99/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0132 - accuracy: 0.9921 - val_loss: 1.3104 - val_accuracy: 0.7791\n",
      "Epoch 100/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0136 - accuracy: 0.9922 - val_loss: 1.3172 - val_accuracy: 0.7739\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15e633c50>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_3.fit(X_train_padded_seqs, one_hot_labels,  epochs=100, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78ddbf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.7693677649154052\n",
      "平均f1-score: 0.7702834181445732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        54\n",
      "           1       0.74      0.70      0.72        79\n",
      "           2       0.79      0.59      0.68        32\n",
      "           3       0.78      1.00      0.88        18\n",
      "           4       0.92      0.89      0.90       166\n",
      "           5       0.87      0.84      0.85        87\n",
      "           6       0.77      0.73      0.75       140\n",
      "           7       0.81      0.64      0.71        39\n",
      "           8       0.33      0.62      0.43        13\n",
      "           9       0.89      0.85      0.87       111\n",
      "          10       0.75      0.86      0.80        28\n",
      "          11       0.79      0.73      0.76       135\n",
      "          12       0.74      0.89      0.81        28\n",
      "          13       0.84      0.84      0.84        51\n",
      "          14       0.75      0.75      0.75         4\n",
      "          15       0.60      0.66      0.63        32\n",
      "          16       0.89      0.88      0.88       177\n",
      "          17       0.83      0.88      0.86       104\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.86      0.89      0.88        28\n",
      "          20       0.82      0.94      0.88        69\n",
      "          21       0.68      0.71      0.70        76\n",
      "          22       0.70      0.50      0.58       133\n",
      "          23       0.63      0.65      0.64       140\n",
      "          24       0.82      0.82      0.82        56\n",
      "          25       0.79      0.79      0.79       137\n",
      "          26       0.59      0.65      0.62        26\n",
      "          27       0.69      1.00      0.82         9\n",
      "          28       0.87      0.89      0.88       121\n",
      "          29       0.66      0.71      0.68        65\n",
      "          30       0.30      0.60      0.40         5\n",
      "          31       0.97      0.98      0.97        57\n",
      "          32       0.75      1.00      0.86         3\n",
      "          33       0.85      0.96      0.90        23\n",
      "          34       0.93      0.90      0.91        29\n",
      "          35       0.50      1.00      0.67         3\n",
      "          36       0.58      0.44      0.50        34\n",
      "          37       0.84      0.83      0.83       111\n",
      "          38       0.67      0.86      0.75         7\n",
      "          39       0.87      0.87      0.87       126\n",
      "          40       0.95      0.95      0.95        41\n",
      "          41       1.00      1.00      1.00         4\n",
      "          42       0.67      1.00      0.80         8\n",
      "          43       0.79      0.69      0.73        16\n",
      "          44       0.96      0.89      0.92        27\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       0.56      0.85      0.68        26\n",
      "          47       0.83      0.83      0.83        12\n",
      "          48       0.92      0.85      0.88        13\n",
      "          49       0.89      0.89      0.89        18\n",
      "          50       0.56      0.63      0.59        35\n",
      "          51       0.64      0.84      0.73        19\n",
      "          52       0.20      0.25      0.22         8\n",
      "          53       1.00      1.00      1.00         7\n",
      "          54       0.66      0.53      0.59        94\n",
      "          55       0.67      1.00      0.80         2\n",
      "          56       0.75      0.33      0.46         9\n",
      "          57       1.00      0.80      0.89         5\n",
      "          58       0.94      1.00      0.97        15\n",
      "          59       0.86      1.00      0.92         6\n",
      "          60       0.40      0.40      0.40         5\n",
      "          61       0.77      0.68      0.72        25\n",
      "          62       0.83      0.78      0.81        37\n",
      "          63       0.73      0.73      0.73        37\n",
      "          64       0.68      0.81      0.74        52\n",
      "          65       0.65      0.75      0.70        20\n",
      "          66       0.86      0.86      0.86       108\n",
      "          67       1.00      0.94      0.97        17\n",
      "          68       0.43      0.27      0.33        11\n",
      "          69       0.25      0.25      0.25         4\n",
      "          70       0.67      0.48      0.56        29\n",
      "          71       0.65      0.66      0.65        85\n",
      "          72       0.71      0.91      0.80        11\n",
      "          73       0.71      0.45      0.56        11\n",
      "          74       0.74      0.67      0.70        84\n",
      "          75       0.79      0.71      0.75        31\n",
      "          76       0.54      0.60      0.57        25\n",
      "          77       0.90      0.97      0.93        29\n",
      "          78       0.62      0.83      0.71         6\n",
      "          79       0.93      1.00      0.96        27\n",
      "          80       0.85      1.00      0.92        22\n",
      "          81       0.62      0.57      0.59        14\n",
      "          82       0.86      0.75      0.80        16\n",
      "          83       0.50      1.00      0.67         3\n",
      "          84       0.75      0.75      0.75         8\n",
      "          85       0.62      0.63      0.63        84\n",
      "          86       1.00      1.00      1.00         1\n",
      "          87       0.77      0.83      0.80        60\n",
      "          88       0.82      0.82      0.82        11\n",
      "          89       0.86      0.86      0.86        35\n",
      "          90       0.86      0.97      0.91        38\n",
      "          91       0.90      0.90      0.90        10\n",
      "          92       0.25      0.40      0.31        10\n",
      "          93       0.67      0.50      0.57        24\n",
      "          94       0.56      0.83      0.67        12\n",
      "          95       0.83      0.97      0.89        30\n",
      "          96       0.45      0.62      0.53         8\n",
      "          97       0.73      1.00      0.84         8\n",
      "          98       0.84      0.91      0.88        47\n",
      "          99       0.82      0.61      0.70       137\n",
      "         100       0.82      1.00      0.90         9\n",
      "         101       0.94      0.88      0.91        34\n",
      "         102       0.67      1.00      0.80         4\n",
      "         103       0.71      0.55      0.62       109\n",
      "         104       0.12      0.20      0.15         5\n",
      "         106       0.33      1.00      0.50         3\n",
      "         107       0.88      0.92      0.90        38\n",
      "         108       0.47      0.70      0.56        10\n",
      "         109       0.43      1.00      0.60         3\n",
      "         110       0.71      0.79      0.75        19\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.76      0.86      0.81        22\n",
      "         113       0.33      1.00      0.50         1\n",
      "         114       0.00      0.00      0.00         2\n",
      "         115       0.60      0.67      0.63         9\n",
      "         116       0.00      0.00      0.00         0\n",
      "         117       0.00      0.00      0.00         3\n",
      "         118       1.00      1.00      1.00         2\n",
      "         119       0.45      0.62      0.53         8\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.68      0.93      0.79        14\n",
      "         122       0.14      0.12      0.13         8\n",
      "         123       0.00      0.00      0.00         4\n",
      "         124       0.83      1.00      0.91         5\n",
      "         125       0.25      0.12      0.17         8\n",
      "         126       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "         130       0.00      0.00      0.00         0\n",
      "         131       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.77      4492\n",
      "   macro avg       0.66      0.71      0.68      4492\n",
      "weighted avg       0.78      0.77      0.77      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_3.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201cd65b",
   "metadata": {},
   "source": [
    "### 加入word2vec初始化embedding层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd80148e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "x_train = np.concatenate((X))\n",
    "w2v_model = Word2Vec(vector_size=300)\n",
    "w2v_model.build_vocab(x_train)\n",
    "w2v_model.train(x_train, total_examples = w2v_model.corpus_count, epochs = 200)\n",
    "w2v_model.save('w2v_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "95157b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec.load('w2v_model.pkl')\n",
    "# 预训练的词向量中没有出现的词用0向量表示\n",
    "embedding_matrix = np.zeros((len(vocab) + 1, 300))\n",
    "for word, i in vocab.items():\n",
    "    try:\n",
    "        embedding_vector = w2v_model.wv[str(word)]\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "    except KeyError:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea916030",
   "metadata": {},
   "source": [
    "#### 一级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f8aa6985",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['一级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5664f599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 30, 300)      3864600     input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_9 (Conv1D)               (None, 30, 32)       28832       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 30, 32)       38432       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 30, 32)       48032       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_9 (MaxPooling1D)  (None, 1, 32)        0           conv1d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_10 (MaxPooling1D) (None, 1, 32)        0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_11 (MaxPooling1D) (None, 1, 32)        0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, 1, 96)        0           max_pooling1d_9[0][0]            \n",
      "                                                                 max_pooling1d_10[0][0]           \n",
      "                                                                 max_pooling1d_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_3 (Flatten)             (None, 96)           0           concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 96)           0           flatten_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 23)           2231        dropout_3[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 3,982,127\n",
      "Trainable params: 3,982,127\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(32, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(32, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(32, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_W2V_1 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_W2V_1.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_W2V_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9e438201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/20\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 2.5410 - accuracy: 0.3167 - val_loss: 2.0383 - val_accuracy: 0.4509\n",
      "Epoch 2/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 1.8596 - accuracy: 0.4651 - val_loss: 1.6717 - val_accuracy: 0.5496\n",
      "Epoch 3/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 1.4284 - accuracy: 0.6183 - val_loss: 1.3095 - val_accuracy: 0.6617\n",
      "Epoch 4/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.9891 - accuracy: 0.7637 - val_loss: 1.0092 - val_accuracy: 0.7591\n",
      "Epoch 5/20\n",
      "8383/8383 [==============================] - 11s 1ms/sample - loss: 0.6554 - accuracy: 0.8572 - val_loss: 0.8127 - val_accuracy: 0.8139\n",
      "Epoch 6/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.4410 - accuracy: 0.9128 - val_loss: 0.6928 - val_accuracy: 0.8306\n",
      "Epoch 7/20\n",
      "8383/8383 [==============================] - 11s 1ms/sample - loss: 0.3038 - accuracy: 0.9418 - val_loss: 0.6173 - val_accuracy: 0.8540\n",
      "Epoch 8/20\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.2188 - accuracy: 0.9618 - val_loss: 0.5729 - val_accuracy: 0.8616\n",
      "Epoch 9/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.1608 - accuracy: 0.9717 - val_loss: 0.5427 - val_accuracy: 0.8650\n",
      "Epoch 10/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.1251 - accuracy: 0.9765 - val_loss: 0.5251 - val_accuracy: 0.8750\n",
      "Epoch 11/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0982 - accuracy: 0.9815 - val_loss: 0.5139 - val_accuracy: 0.8750\n",
      "Epoch 12/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0812 - accuracy: 0.9839 - val_loss: 0.5053 - val_accuracy: 0.8779\n",
      "Epoch 13/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0691 - accuracy: 0.9864 - val_loss: 0.4976 - val_accuracy: 0.8793\n",
      "Epoch 14/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0577 - accuracy: 0.9894 - val_loss: 0.4912 - val_accuracy: 0.8807\n",
      "Epoch 15/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0492 - accuracy: 0.9909 - val_loss: 0.4927 - val_accuracy: 0.8826\n",
      "Epoch 16/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0449 - accuracy: 0.9908 - val_loss: 0.4942 - val_accuracy: 0.8826\n",
      "Epoch 17/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0389 - accuracy: 0.9918 - val_loss: 0.4922 - val_accuracy: 0.8831\n",
      "Epoch 18/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0356 - accuracy: 0.9933 - val_loss: 0.4986 - val_accuracy: 0.8860\n",
      "Epoch 19/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0305 - accuracy: 0.9945 - val_loss: 0.4964 - val_accuracy: 0.8850\n",
      "Epoch 20/20\n",
      "8383/8383 [==============================] - 10s 1ms/sample - loss: 0.0266 - accuracy: 0.9956 - val_loss: 0.5012 - val_accuracy: 0.8841\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16dddac10>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_W2V_1.fit(X_train_padded_seqs, one_hot_labels,  epochs=20, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973b02cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.8862422083704363\n",
      "平均f1-score: 0.8829398976340694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.98      0.93       121\n",
      "           1       0.92      0.90      0.91       471\n",
      "           2       0.97      0.87      0.92       219\n",
      "           3       0.96      0.90      0.93      1447\n",
      "           4       0.90      0.87      0.89       228\n",
      "           5       0.90      0.89      0.90       342\n",
      "           6       0.87      0.88      0.88       441\n",
      "           7       0.71      0.84      0.77       100\n",
      "           8       0.88      0.91      0.90       156\n",
      "           9       0.85      0.89      0.87       364\n",
      "          10       0.60      0.79      0.68        58\n",
      "          11       0.82      0.81      0.81       220\n",
      "          12       0.82      0.78      0.80        60\n",
      "          13       0.89      1.00      0.94        31\n",
      "          14       0.69      0.70      0.69        53\n",
      "          15       0.70      0.91      0.79        34\n",
      "          16       0.10      0.50      0.17         2\n",
      "          17       0.86      0.90      0.88       121\n",
      "          18       0.64      0.84      0.73        19\n",
      "          19       0.36      1.00      0.53         4\n",
      "          20       0.00      0.00      0.00         0\n",
      "          21       0.00      0.00      0.00         1\n",
      "          22       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.89      4492\n",
      "   macro avg       0.67      0.75      0.69      4492\n",
      "weighted avg       0.90      0.89      0.89      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_W2V_1.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c07d01",
   "metadata": {},
   "source": [
    "#### 二级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "89d458f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['二级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cae1b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_5 (Embedding)         (None, 30, 300)      3864600     input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 30, 64)       57664       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 30, 64)       76864       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_14 (Conv1D)              (None, 30, 64)       96064       embedding_5[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_12 (MaxPooling1D) (None, 1, 64)        0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_13 (MaxPooling1D) (None, 1, 64)        0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_14 (MaxPooling1D) (None, 1, 64)        0           conv1d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 1, 192)       0           max_pooling1d_12[0][0]           \n",
      "                                                                 max_pooling1d_13[0][0]           \n",
      "                                                                 max_pooling1d_14[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 192)          0           concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 192)          0           flatten_4[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 84)           16212       dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,111,404\n",
      "Trainable params: 4,111,404\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(64, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(64, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(64, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_W2V_2 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_W2V_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_W2V_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4f11b15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/50\n",
      "8383/8383 [==============================] - 15s 2ms/sample - loss: 3.8249 - accuracy: 0.1754 - val_loss: 3.3446 - val_accuracy: 0.2481\n",
      "Epoch 2/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 3.0774 - accuracy: 0.3107 - val_loss: 2.9182 - val_accuracy: 0.3569\n",
      "Epoch 3/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 2.5815 - accuracy: 0.4321 - val_loss: 2.5452 - val_accuracy: 0.4552\n",
      "Epoch 4/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 2.0794 - accuracy: 0.5808 - val_loss: 2.1689 - val_accuracy: 0.5596\n",
      "Epoch 5/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 1.5775 - accuracy: 0.7025 - val_loss: 1.8077 - val_accuracy: 0.6417\n",
      "Epoch 6/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 1.1340 - accuracy: 0.7947 - val_loss: 1.5365 - val_accuracy: 0.6947\n",
      "Epoch 7/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.7998 - accuracy: 0.8510 - val_loss: 1.3550 - val_accuracy: 0.7252\n",
      "Epoch 8/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.5826 - accuracy: 0.8899 - val_loss: 1.2244 - val_accuracy: 0.7438\n",
      "Epoch 9/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.4334 - accuracy: 0.9194 - val_loss: 1.1462 - val_accuracy: 0.7548\n",
      "Epoch 10/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.3257 - accuracy: 0.9351 - val_loss: 1.0883 - val_accuracy: 0.7667\n",
      "Epoch 11/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.2495 - accuracy: 0.9535 - val_loss: 1.0439 - val_accuracy: 0.7743\n",
      "Epoch 12/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.2013 - accuracy: 0.9628 - val_loss: 1.0200 - val_accuracy: 0.7796\n",
      "Epoch 13/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.1655 - accuracy: 0.9687 - val_loss: 1.0053 - val_accuracy: 0.7767\n",
      "Epoch 14/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.1383 - accuracy: 0.9724 - val_loss: 0.9888 - val_accuracy: 0.7796\n",
      "Epoch 15/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.1172 - accuracy: 0.9783 - val_loss: 0.9923 - val_accuracy: 0.7839\n",
      "Epoch 16/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0976 - accuracy: 0.9798 - val_loss: 0.9800 - val_accuracy: 0.7829\n",
      "Epoch 17/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0887 - accuracy: 0.9828 - val_loss: 0.9764 - val_accuracy: 0.7920\n",
      "Epoch 18/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0739 - accuracy: 0.9850 - val_loss: 0.9733 - val_accuracy: 0.7944\n",
      "Epoch 19/50\n",
      "8383/8383 [==============================] - 12s 1ms/sample - loss: 0.0695 - accuracy: 0.9863 - val_loss: 0.9675 - val_accuracy: 0.7901\n",
      "Epoch 20/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0654 - accuracy: 0.9860 - val_loss: 0.9678 - val_accuracy: 0.7939\n",
      "Epoch 21/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0605 - accuracy: 0.9869 - val_loss: 0.9686 - val_accuracy: 0.7963\n",
      "Epoch 22/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0542 - accuracy: 0.9875 - val_loss: 0.9725 - val_accuracy: 0.7958\n",
      "Epoch 23/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0491 - accuracy: 0.9901 - val_loss: 0.9738 - val_accuracy: 0.7987\n",
      "Epoch 24/50\n",
      "8383/8383 [==============================] - 13s 1ms/sample - loss: 0.0449 - accuracy: 0.9906 - val_loss: 0.9753 - val_accuracy: 0.7987\n",
      "Epoch 25/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0426 - accuracy: 0.9911 - val_loss: 0.9801 - val_accuracy: 0.7996\n",
      "Epoch 26/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0381 - accuracy: 0.9912 - val_loss: 0.9793 - val_accuracy: 0.7991\n",
      "Epoch 27/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0380 - accuracy: 0.9909 - val_loss: 0.9854 - val_accuracy: 0.8006\n",
      "Epoch 28/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0382 - accuracy: 0.9902 - val_loss: 0.9856 - val_accuracy: 0.8015\n",
      "Epoch 29/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0357 - accuracy: 0.9914 - val_loss: 0.9843 - val_accuracy: 0.8001\n",
      "Epoch 30/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0313 - accuracy: 0.9926 - val_loss: 0.9830 - val_accuracy: 0.8049\n",
      "Epoch 31/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0319 - accuracy: 0.9922 - val_loss: 0.9856 - val_accuracy: 0.8025\n",
      "Epoch 32/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0276 - accuracy: 0.9927 - val_loss: 0.9823 - val_accuracy: 0.8030\n",
      "Epoch 33/50\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 0.0289 - accuracy: 0.9928 - val_loss: 0.9844 - val_accuracy: 0.8044\n",
      "Epoch 34/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0292 - accuracy: 0.9919 - val_loss: 0.9891 - val_accuracy: 0.8030\n",
      "Epoch 35/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0284 - accuracy: 0.9920 - val_loss: 1.0028 - val_accuracy: 0.8015\n",
      "Epoch 36/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0251 - accuracy: 0.9937 - val_loss: 0.9888 - val_accuracy: 0.8034\n",
      "Epoch 37/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0267 - accuracy: 0.9931 - val_loss: 0.9987 - val_accuracy: 0.8058\n",
      "Epoch 38/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0223 - accuracy: 0.9939 - val_loss: 0.9997 - val_accuracy: 0.8030\n",
      "Epoch 39/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0252 - accuracy: 0.9934 - val_loss: 1.0019 - val_accuracy: 0.8096\n",
      "Epoch 40/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0217 - accuracy: 0.9928 - val_loss: 1.0112 - val_accuracy: 0.8053\n",
      "Epoch 41/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0223 - accuracy: 0.9937 - val_loss: 1.0117 - val_accuracy: 0.8096\n",
      "Epoch 42/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0221 - accuracy: 0.9934 - val_loss: 1.0211 - val_accuracy: 0.8034\n",
      "Epoch 43/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0214 - accuracy: 0.9939 - val_loss: 1.0156 - val_accuracy: 0.8025\n",
      "Epoch 44/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0203 - accuracy: 0.9945 - val_loss: 1.0238 - val_accuracy: 0.8068\n",
      "Epoch 45/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0227 - accuracy: 0.9928 - val_loss: 1.0253 - val_accuracy: 0.8020\n",
      "Epoch 46/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0199 - accuracy: 0.9936 - val_loss: 1.0181 - val_accuracy: 0.8039\n",
      "Epoch 47/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0240 - accuracy: 0.9922 - val_loss: 1.0166 - val_accuracy: 0.8044\n",
      "Epoch 48/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0212 - accuracy: 0.9932 - val_loss: 1.0145 - val_accuracy: 0.8096\n",
      "Epoch 49/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0193 - accuracy: 0.9930 - val_loss: 1.0207 - val_accuracy: 0.8096\n",
      "Epoch 50/50\n",
      "8383/8383 [==============================] - 13s 2ms/sample - loss: 0.0186 - accuracy: 0.9934 - val_loss: 1.0246 - val_accuracy: 0.8068\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16ddebf10>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_W2V_2.fit(X_train_padded_seqs, one_hot_labels,  epochs=50, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6e9f3a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.7983081032947462\n",
      "平均f1-score: 0.7958470832828048\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.98      0.96        53\n",
      "           1       0.87      0.70      0.78        98\n",
      "           2       0.98      0.85      0.91       212\n",
      "           3       0.86      0.92      0.89        48\n",
      "           4       0.89      0.72      0.80       154\n",
      "           5       0.73      0.72      0.72       135\n",
      "           6       0.85      0.93      0.89        60\n",
      "           7       0.91      0.83      0.87       115\n",
      "           8       0.84      0.80      0.82        54\n",
      "           9       0.88      0.86      0.87       221\n",
      "          10       0.88      0.89      0.88        55\n",
      "          11       0.80      0.82      0.81        50\n",
      "          12       0.79      0.81      0.80        57\n",
      "          13       0.67      0.58      0.62        71\n",
      "          14       0.89      0.91      0.90       171\n",
      "          15       0.83      0.84      0.83       110\n",
      "          16       1.00      0.80      0.89        15\n",
      "          17       0.83      0.86      0.84        28\n",
      "          18       0.70      0.80      0.74        69\n",
      "          19       0.63      0.47      0.54       126\n",
      "          20       0.65      0.61      0.63       158\n",
      "          21       0.79      0.88      0.83        56\n",
      "          22       0.81      0.80      0.80       139\n",
      "          23       0.45      0.65      0.53        20\n",
      "          24       0.46      0.75      0.57         8\n",
      "          25       0.85      0.89      0.87       119\n",
      "          26       0.61      0.70      0.66        61\n",
      "          27       0.20      0.50      0.29         4\n",
      "          28       0.95      0.92      0.94        64\n",
      "          29       0.77      0.87      0.82        23\n",
      "          30       0.89      0.83      0.86        30\n",
      "          31       0.50      0.75      0.60         4\n",
      "          32       0.92      0.85      0.88       327\n",
      "          33       0.67      1.00      0.80         6\n",
      "          34       0.84      0.78      0.81       179\n",
      "          35       0.87      0.88      0.88        52\n",
      "          36       0.72      0.85      0.78        34\n",
      "          37       0.94      0.97      0.95        32\n",
      "          38       0.56      0.77      0.65        31\n",
      "          39       0.84      0.90      0.87        29\n",
      "          40       0.54      0.75      0.63        28\n",
      "          41       0.80      0.92      0.86        66\n",
      "          42       0.20      0.40      0.27         5\n",
      "          43       0.66      0.67      0.66        75\n",
      "          44       0.64      0.88      0.74         8\n",
      "          45       0.75      0.33      0.46         9\n",
      "          46       0.87      0.82      0.85       107\n",
      "          47       0.65      0.75      0.70        32\n",
      "          48       0.68      0.75      0.71        56\n",
      "          49       0.43      0.43      0.43         7\n",
      "          50       0.00      0.00      0.00         2\n",
      "          51       0.62      0.59      0.60        22\n",
      "          52       0.68      0.78      0.72        76\n",
      "          53       0.64      0.64      0.64        14\n",
      "          54       0.81      0.82      0.82       157\n",
      "          55       0.50      0.61      0.55        23\n",
      "          56       0.97      1.00      0.98        28\n",
      "          57       0.76      0.94      0.84        36\n",
      "          58       0.93      0.87      0.90        15\n",
      "          59       0.57      0.80      0.67        10\n",
      "          60       0.85      0.78      0.81       183\n",
      "          61       0.77      0.97      0.86        34\n",
      "          62       0.91      0.91      0.91        11\n",
      "          63       0.83      0.65      0.73        23\n",
      "          64       0.56      0.77      0.65        13\n",
      "          65       0.45      0.71      0.56         7\n",
      "          66       0.82      1.00      0.90         9\n",
      "          67       0.88      0.91      0.90        80\n",
      "          68       0.89      0.80      0.84        10\n",
      "          69       0.00      0.00      0.00         3\n",
      "          71       0.33      1.00      0.50         3\n",
      "          72       0.67      0.82      0.74        17\n",
      "          73       0.76      0.86      0.81        22\n",
      "          74       0.54      0.64      0.58        11\n",
      "          75       0.00      0.00      0.00         0\n",
      "          76       0.00      0.00      0.00         2\n",
      "          77       0.50      1.00      0.67         1\n",
      "          78       0.00      0.00      0.00         0\n",
      "          79       0.14      0.20      0.17         5\n",
      "          80       0.25      0.25      0.25         4\n",
      "          82       0.00      0.00      0.00         0\n",
      "          83       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.80      4492\n",
      "   macro avg       0.66      0.71      0.67      4492\n",
      "weighted avg       0.81      0.80      0.80      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_W2V_2.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9c05a0",
   "metadata": {},
   "source": [
    "#### 三级标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "33024f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = data['三级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a32f1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 30, 300)      3864600     input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_15 (Conv1D)              (None, 30, 128)      115328      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_16 (Conv1D)              (None, 30, 128)      153728      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_17 (Conv1D)              (None, 30, 128)      192128      embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_15 (MaxPooling1D) (None, 1, 128)       0           conv1d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_16 (MaxPooling1D) (None, 1, 128)       0           conv1d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_17 (MaxPooling1D) (None, 1, 128)       0           conv1d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 1, 384)       0           max_pooling1d_15[0][0]           \n",
      "                                                                 max_pooling1d_16[0][0]           \n",
      "                                                                 max_pooling1d_17[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)             (None, 384)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_5 (Dropout)             (None, 384)          0           flatten_5[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 133)          51205       dropout_5[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,376,989\n",
      "Trainable params: 4,376,989\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen, weights=[embedding_matrix], trainable=True)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(128, 3, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(128, 4, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(128, 5, padding='same', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_W2V_3 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_W2V_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_W2V_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "63e9984a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/100\n",
      "8383/8383 [==============================] - 18s 2ms/sample - loss: 4.1510 - accuracy: 0.1705 - val_loss: 3.6532 - val_accuracy: 0.2595\n",
      "Epoch 2/100\n",
      "8383/8383 [==============================] - 19s 2ms/sample - loss: 3.3528 - accuracy: 0.3115 - val_loss: 3.2372 - val_accuracy: 0.3473\n",
      "Epoch 3/100\n",
      "8383/8383 [==============================] - 21s 3ms/sample - loss: 2.8463 - accuracy: 0.4234 - val_loss: 2.8742 - val_accuracy: 0.4427\n",
      "Epoch 4/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 2.3288 - accuracy: 0.5436 - val_loss: 2.4835 - val_accuracy: 0.5267\n",
      "Epoch 5/100\n",
      "8383/8383 [==============================] - 20s 2ms/sample - loss: 1.7835 - accuracy: 0.6680 - val_loss: 2.1108 - val_accuracy: 0.5883\n",
      "Epoch 6/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 1.2868 - accuracy: 0.7679 - val_loss: 1.8081 - val_accuracy: 0.6388\n",
      "Epoch 7/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.9132 - accuracy: 0.8347 - val_loss: 1.6089 - val_accuracy: 0.6842\n",
      "Epoch 8/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.6515 - accuracy: 0.8819 - val_loss: 1.4735 - val_accuracy: 0.6951\n",
      "Epoch 9/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.4712 - accuracy: 0.9159 - val_loss: 1.4000 - val_accuracy: 0.7080\n",
      "Epoch 10/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.3475 - accuracy: 0.9367 - val_loss: 1.3394 - val_accuracy: 0.7195\n",
      "Epoch 11/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.2679 - accuracy: 0.9466 - val_loss: 1.2985 - val_accuracy: 0.7314\n",
      "Epoch 12/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.2067 - accuracy: 0.9625 - val_loss: 1.2785 - val_accuracy: 0.7343\n",
      "Epoch 13/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.1630 - accuracy: 0.9683 - val_loss: 1.2590 - val_accuracy: 0.7362\n",
      "Epoch 14/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.1370 - accuracy: 0.9740 - val_loss: 1.2601 - val_accuracy: 0.7457\n",
      "Epoch 15/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.1153 - accuracy: 0.9777 - val_loss: 1.2585 - val_accuracy: 0.7476\n",
      "Epoch 16/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.1011 - accuracy: 0.9794 - val_loss: 1.2404 - val_accuracy: 0.7490\n",
      "Epoch 17/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0883 - accuracy: 0.9814 - val_loss: 1.2418 - val_accuracy: 0.7476\n",
      "Epoch 18/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0778 - accuracy: 0.9837 - val_loss: 1.2509 - val_accuracy: 0.7438\n",
      "Epoch 19/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0698 - accuracy: 0.9840 - val_loss: 1.2602 - val_accuracy: 0.7538\n",
      "Epoch 20/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0641 - accuracy: 0.9858 - val_loss: 1.2484 - val_accuracy: 0.7519\n",
      "Epoch 21/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0569 - accuracy: 0.9863 - val_loss: 1.2502 - val_accuracy: 0.7557\n",
      "Epoch 22/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0548 - accuracy: 0.9880 - val_loss: 1.2706 - val_accuracy: 0.7567\n",
      "Epoch 23/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0488 - accuracy: 0.9889 - val_loss: 1.2748 - val_accuracy: 0.7600\n",
      "Epoch 24/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0443 - accuracy: 0.9902 - val_loss: 1.2829 - val_accuracy: 0.7605\n",
      "Epoch 25/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0470 - accuracy: 0.9876 - val_loss: 1.2688 - val_accuracy: 0.7591\n",
      "Epoch 26/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0417 - accuracy: 0.9888 - val_loss: 1.2758 - val_accuracy: 0.7615\n",
      "Epoch 27/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0385 - accuracy: 0.9908 - val_loss: 1.2857 - val_accuracy: 0.7576\n",
      "Epoch 28/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0394 - accuracy: 0.9890 - val_loss: 1.2863 - val_accuracy: 0.7572\n",
      "Epoch 29/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0357 - accuracy: 0.9907 - val_loss: 1.2951 - val_accuracy: 0.7572\n",
      "Epoch 30/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0325 - accuracy: 0.9909 - val_loss: 1.2866 - val_accuracy: 0.7610\n",
      "Epoch 31/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0332 - accuracy: 0.9899 - val_loss: 1.2922 - val_accuracy: 0.7653\n",
      "Epoch 32/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0313 - accuracy: 0.9905 - val_loss: 1.3011 - val_accuracy: 0.7595\n",
      "Epoch 33/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0321 - accuracy: 0.9905 - val_loss: 1.3168 - val_accuracy: 0.7610\n",
      "Epoch 34/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0313 - accuracy: 0.9913 - val_loss: 1.3146 - val_accuracy: 0.7624\n",
      "Epoch 35/100\n",
      "8383/8383 [==============================] - 18s 2ms/sample - loss: 0.0288 - accuracy: 0.9924 - val_loss: 1.3086 - val_accuracy: 0.7638\n",
      "Epoch 36/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0281 - accuracy: 0.9909 - val_loss: 1.3048 - val_accuracy: 0.7677\n",
      "Epoch 37/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0297 - accuracy: 0.9915 - val_loss: 1.3162 - val_accuracy: 0.7634\n",
      "Epoch 38/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0280 - accuracy: 0.9903 - val_loss: 1.3118 - val_accuracy: 0.7638\n",
      "Epoch 39/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0267 - accuracy: 0.9912 - val_loss: 1.3211 - val_accuracy: 0.7615\n",
      "Epoch 40/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0291 - accuracy: 0.9908 - val_loss: 1.3192 - val_accuracy: 0.7624\n",
      "Epoch 41/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0280 - accuracy: 0.9916 - val_loss: 1.3389 - val_accuracy: 0.7605\n",
      "Epoch 42/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0255 - accuracy: 0.9918 - val_loss: 1.3285 - val_accuracy: 0.7619\n",
      "Epoch 43/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0240 - accuracy: 0.9924 - val_loss: 1.3416 - val_accuracy: 0.7643\n",
      "Epoch 44/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0268 - accuracy: 0.9911 - val_loss: 1.3397 - val_accuracy: 0.7624\n",
      "Epoch 45/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0239 - accuracy: 0.9919 - val_loss: 1.3312 - val_accuracy: 0.7657\n",
      "Epoch 46/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0259 - accuracy: 0.9914 - val_loss: 1.3395 - val_accuracy: 0.7615\n",
      "Epoch 47/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0218 - accuracy: 0.9924 - val_loss: 1.3372 - val_accuracy: 0.7662\n",
      "Epoch 48/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0224 - accuracy: 0.9915 - val_loss: 1.3388 - val_accuracy: 0.7619\n",
      "Epoch 49/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0222 - accuracy: 0.9921 - val_loss: 1.3605 - val_accuracy: 0.7610\n",
      "Epoch 50/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0267 - accuracy: 0.9922 - val_loss: 1.3461 - val_accuracy: 0.7615\n",
      "Epoch 51/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0226 - accuracy: 0.9916 - val_loss: 1.3539 - val_accuracy: 0.7605\n",
      "Epoch 52/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0233 - accuracy: 0.9920 - val_loss: 1.3508 - val_accuracy: 0.7634\n",
      "Epoch 53/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0239 - accuracy: 0.9920 - val_loss: 1.3557 - val_accuracy: 0.7610\n",
      "Epoch 54/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0215 - accuracy: 0.9919 - val_loss: 1.3487 - val_accuracy: 0.7634\n",
      "Epoch 55/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0231 - accuracy: 0.9913 - val_loss: 1.3673 - val_accuracy: 0.7619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0215 - accuracy: 0.9932 - val_loss: 1.3564 - val_accuracy: 0.7600\n",
      "Epoch 57/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0196 - accuracy: 0.9930 - val_loss: 1.3669 - val_accuracy: 0.7595\n",
      "Epoch 58/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0213 - accuracy: 0.9914 - val_loss: 1.3776 - val_accuracy: 0.7591\n",
      "Epoch 59/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0233 - accuracy: 0.9916 - val_loss: 1.3763 - val_accuracy: 0.7600\n",
      "Epoch 60/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0191 - accuracy: 0.9928 - val_loss: 1.3794 - val_accuracy: 0.7634\n",
      "Epoch 61/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0184 - accuracy: 0.9926 - val_loss: 1.3745 - val_accuracy: 0.7648\n",
      "Epoch 62/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0223 - accuracy: 0.9920 - val_loss: 1.3790 - val_accuracy: 0.7619\n",
      "Epoch 63/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0208 - accuracy: 0.9928 - val_loss: 1.3764 - val_accuracy: 0.7624\n",
      "Epoch 64/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0209 - accuracy: 0.9916 - val_loss: 1.3934 - val_accuracy: 0.7638\n",
      "Epoch 65/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0218 - accuracy: 0.9924 - val_loss: 1.3857 - val_accuracy: 0.7591\n",
      "Epoch 66/100\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 0.0214 - accuracy: 0.9921 - val_loss: 1.3981 - val_accuracy: 0.7624\n",
      "Epoch 67/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0202 - accuracy: 0.9918 - val_loss: 1.3901 - val_accuracy: 0.7610\n",
      "Epoch 68/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0209 - accuracy: 0.9915 - val_loss: 1.3941 - val_accuracy: 0.7629\n",
      "Epoch 69/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0208 - accuracy: 0.9916 - val_loss: 1.3927 - val_accuracy: 0.7677\n",
      "Epoch 70/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0224 - accuracy: 0.9918 - val_loss: 1.3821 - val_accuracy: 0.7657\n",
      "Epoch 71/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0194 - accuracy: 0.9921 - val_loss: 1.4070 - val_accuracy: 0.7624\n",
      "Epoch 72/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0215 - accuracy: 0.9912 - val_loss: 1.4124 - val_accuracy: 0.7610\n",
      "Epoch 73/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0198 - accuracy: 0.9921 - val_loss: 1.4049 - val_accuracy: 0.7610\n",
      "Epoch 74/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0197 - accuracy: 0.9927 - val_loss: 1.4062 - val_accuracy: 0.7648\n",
      "Epoch 75/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0195 - accuracy: 0.9915 - val_loss: 1.4114 - val_accuracy: 0.7615\n",
      "Epoch 76/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0223 - accuracy: 0.9911 - val_loss: 1.4248 - val_accuracy: 0.7576\n",
      "Epoch 77/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0196 - accuracy: 0.9924 - val_loss: 1.4090 - val_accuracy: 0.7624\n",
      "Epoch 78/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0204 - accuracy: 0.9913 - val_loss: 1.4004 - val_accuracy: 0.7615\n",
      "Epoch 79/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0192 - accuracy: 0.9930 - val_loss: 1.3991 - val_accuracy: 0.7629\n",
      "Epoch 80/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0207 - accuracy: 0.9918 - val_loss: 1.3976 - val_accuracy: 0.7610\n",
      "Epoch 81/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0184 - accuracy: 0.9921 - val_loss: 1.4117 - val_accuracy: 0.7619\n",
      "Epoch 82/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0189 - accuracy: 0.9921 - val_loss: 1.4114 - val_accuracy: 0.7576\n",
      "Epoch 83/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0174 - accuracy: 0.9918 - val_loss: 1.4166 - val_accuracy: 0.7591\n",
      "Epoch 84/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0185 - accuracy: 0.9931 - val_loss: 1.4085 - val_accuracy: 0.7634\n",
      "Epoch 85/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0203 - accuracy: 0.9922 - val_loss: 1.4178 - val_accuracy: 0.7629\n",
      "Epoch 86/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0193 - accuracy: 0.9926 - val_loss: 1.4214 - val_accuracy: 0.7634\n",
      "Epoch 87/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0189 - accuracy: 0.9921 - val_loss: 1.4170 - val_accuracy: 0.7615\n",
      "Epoch 88/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0159 - accuracy: 0.9934 - val_loss: 1.4185 - val_accuracy: 0.7600\n",
      "Epoch 89/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0180 - accuracy: 0.9919 - val_loss: 1.4195 - val_accuracy: 0.7634\n",
      "Epoch 90/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0149 - accuracy: 0.9925 - val_loss: 1.4173 - val_accuracy: 0.7657\n",
      "Epoch 91/100\n",
      "8383/8383 [==============================] - 18s 2ms/sample - loss: 0.0165 - accuracy: 0.9931 - val_loss: 1.4338 - val_accuracy: 0.7619\n",
      "Epoch 92/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0171 - accuracy: 0.9924 - val_loss: 1.4315 - val_accuracy: 0.7643\n",
      "Epoch 93/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0148 - accuracy: 0.9927 - val_loss: 1.4291 - val_accuracy: 0.7643\n",
      "Epoch 94/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0172 - accuracy: 0.9916 - val_loss: 1.4325 - val_accuracy: 0.7638\n",
      "Epoch 95/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0175 - accuracy: 0.9922 - val_loss: 1.4403 - val_accuracy: 0.7605\n",
      "Epoch 96/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0176 - accuracy: 0.9925 - val_loss: 1.4375 - val_accuracy: 0.7662\n",
      "Epoch 97/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0156 - accuracy: 0.9932 - val_loss: 1.4391 - val_accuracy: 0.7653\n",
      "Epoch 98/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0181 - accuracy: 0.9916 - val_loss: 1.4320 - val_accuracy: 0.7672\n",
      "Epoch 99/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0162 - accuracy: 0.9919 - val_loss: 1.4506 - val_accuracy: 0.7662\n",
      "Epoch 100/100\n",
      "8383/8383 [==============================] - 17s 2ms/sample - loss: 0.0181 - accuracy: 0.9920 - val_loss: 1.4502 - val_accuracy: 0.7586\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x18c215e50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_W2V_3.fit(X_train_padded_seqs, one_hot_labels,  epochs=100, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e4e75eef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.7615761353517364\n",
      "平均f1-score: 0.7590528214009492\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99        54\n",
      "           1       0.80      0.71      0.75        83\n",
      "           2       0.79      0.76      0.78        25\n",
      "           3       0.74      0.89      0.81        19\n",
      "           4       0.97      0.81      0.88       191\n",
      "           5       0.86      0.83      0.84        87\n",
      "           6       0.76      0.69      0.72       146\n",
      "           7       0.77      0.71      0.74        34\n",
      "           8       0.33      0.62      0.43        13\n",
      "           9       0.92      0.78      0.84       124\n",
      "          10       0.78      0.83      0.81        30\n",
      "          11       0.84      0.70      0.77       151\n",
      "          12       0.71      0.77      0.74        31\n",
      "          13       0.86      0.86      0.86        51\n",
      "          14       0.75      0.75      0.75         4\n",
      "          15       0.54      0.58      0.56        33\n",
      "          16       0.89      0.95      0.92       165\n",
      "          17       0.82      0.81      0.81       113\n",
      "          18       1.00      0.80      0.89        15\n",
      "          19       0.79      0.92      0.85        25\n",
      "          20       0.82      0.87      0.84        75\n",
      "          21       0.71      0.71      0.71        79\n",
      "          22       0.62      0.48      0.54       120\n",
      "          23       0.61      0.67      0.64       133\n",
      "          24       0.84      0.87      0.85        54\n",
      "          25       0.83      0.84      0.84       136\n",
      "          26       0.45      0.45      0.45        29\n",
      "          27       0.46      0.86      0.60         7\n",
      "          28       0.86      0.88      0.87       121\n",
      "          29       0.63      0.76      0.69        58\n",
      "          30       0.20      0.67      0.31         3\n",
      "          31       0.97      0.81      0.88        69\n",
      "          32       0.75      1.00      0.86         3\n",
      "          33       0.73      0.90      0.81        21\n",
      "          34       0.93      0.79      0.85        33\n",
      "          35       0.50      0.75      0.60         4\n",
      "          36       0.73      0.58      0.64        33\n",
      "          37       0.81      0.82      0.81       109\n",
      "          38       0.67      0.86      0.75         7\n",
      "          39       0.87      0.81      0.84       134\n",
      "          40       0.95      0.91      0.93        43\n",
      "          41       1.00      0.80      0.89         5\n",
      "          42       0.50      1.00      0.67         6\n",
      "          43       0.71      0.83      0.77        12\n",
      "          44       0.96      0.89      0.92        27\n",
      "          45       0.67      0.80      0.73         5\n",
      "          46       0.56      0.76      0.65        29\n",
      "          47       0.75      0.75      0.75        12\n",
      "          48       0.92      0.73      0.81        15\n",
      "          49       0.78      1.00      0.88        14\n",
      "          50       0.56      0.61      0.59        36\n",
      "          51       0.56      0.78      0.65        18\n",
      "          52       0.20      0.29      0.24         7\n",
      "          53       0.71      1.00      0.83         5\n",
      "          54       0.66      0.64      0.65        78\n",
      "          55       0.33      1.00      0.50         1\n",
      "          56       0.75      0.33      0.46         9\n",
      "          57       0.75      0.43      0.55         7\n",
      "          58       0.94      0.88      0.91        17\n",
      "          59       0.71      1.00      0.83         5\n",
      "          60       0.40      0.40      0.40         5\n",
      "          61       0.77      0.68      0.72        25\n",
      "          62       0.66      0.82      0.73        28\n",
      "          63       0.65      0.75      0.70        32\n",
      "          64       0.68      0.75      0.71        56\n",
      "          65       0.70      0.62      0.65        26\n",
      "          66       0.86      0.89      0.87       105\n",
      "          67       1.00      0.89      0.94        18\n",
      "          68       0.57      0.40      0.47        10\n",
      "          69       0.25      0.33      0.29         3\n",
      "          70       0.67      0.48      0.56        29\n",
      "          71       0.70      0.72      0.71        83\n",
      "          72       0.64      0.56      0.60        16\n",
      "          73       0.86      0.67      0.75         9\n",
      "          74       0.78      0.66      0.72        89\n",
      "          75       0.68      0.79      0.73        24\n",
      "          76       0.50      0.67      0.57        21\n",
      "          77       1.00      0.67      0.81        46\n",
      "          78       0.75      0.75      0.75         8\n",
      "          79       0.97      0.93      0.95        30\n",
      "          80       0.85      1.00      0.92        22\n",
      "          81       0.54      0.70      0.61        10\n",
      "          82       0.71      0.71      0.71        14\n",
      "          83       0.50      1.00      0.67         3\n",
      "          84       0.50      0.80      0.62         5\n",
      "          85       0.61      0.71      0.66        73\n",
      "          86       1.00      1.00      1.00         1\n",
      "          87       0.69      0.87      0.77        52\n",
      "          88       0.82      0.75      0.78        12\n",
      "          89       0.91      0.84      0.88        38\n",
      "          90       0.77      1.00      0.87        33\n",
      "          91       0.90      0.82      0.86        11\n",
      "          92       0.31      0.38      0.34        13\n",
      "          93       0.83      0.68      0.75        22\n",
      "          94       0.61      0.73      0.67        15\n",
      "          95       0.66      0.92      0.77        25\n",
      "          96       0.55      0.86      0.67         7\n",
      "          97       0.64      0.78      0.70         9\n",
      "          98       0.88      0.92      0.90        49\n",
      "          99       0.82      0.64      0.72       131\n",
      "         100       0.91      1.00      0.95        10\n",
      "         101       0.91      0.94      0.92        31\n",
      "         102       0.50      0.60      0.55         5\n",
      "         103       0.67      0.60      0.63        95\n",
      "         104       0.12      0.33      0.18         3\n",
      "         106       0.33      0.75      0.46         4\n",
      "         107       0.85      0.81      0.83        42\n",
      "         108       0.53      0.50      0.52        16\n",
      "         109       0.43      0.50      0.46         6\n",
      "         110       0.62      0.76      0.68        17\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.80      0.87      0.83        23\n",
      "         113       0.33      1.00      0.50         1\n",
      "         114       0.00      0.00      0.00         2\n",
      "         115       0.50      0.62      0.56         8\n",
      "         116       0.00      0.00      0.00         0\n",
      "         117       0.00      0.00      0.00         6\n",
      "         118       0.50      1.00      0.67         1\n",
      "         119       0.45      0.56      0.50         9\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.68      0.87      0.76        15\n",
      "         122       0.14      0.17      0.15         6\n",
      "         123       0.00      0.00      0.00         0\n",
      "         124       0.33      1.00      0.50         2\n",
      "         125       0.25      0.20      0.22         5\n",
      "         126       0.00      0.00      0.00         2\n",
      "         128       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "         130       0.00      0.00      0.00         0\n",
      "         131       0.00      0.00      0.00         1\n",
      "         132       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.76      4492\n",
      "   macro avg       0.63      0.68      0.64      4492\n",
      "weighted avg       0.78      0.76      0.76      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1245: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_W2V_3.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b2159ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Acc = [[0.8684327693677649,\n",
    "  0.8397150489759573,\n",
    "  0.8379341050756901,\n",
    "  0.8343722172751559,\n",
    "  0.8299198575244879,\n",
    "  0.8065449688334817,\n",
    "  0.896260017809439,\n",
    "  0.8873552983081033,\n",
    "  0.8862422083704363],\n",
    " [0.7640249332146037,\n",
    "  0.7595725734639359,\n",
    "  0.7611308993766697,\n",
    "  0.7377560106856634,\n",
    "  0.7524487978628673,\n",
    "  0.7377560106856634,\n",
    "  0.8118878005342832,\n",
    "  0.81233303650935,\n",
    "  0.7983081032947462],\n",
    " [0.7272929652715939,\n",
    "  0.7361976847729297,\n",
    "  0.7099287622439893,\n",
    "  0.6930097951914514,\n",
    "  0.7281834372217275,\n",
    "  0.6930097951914514,\n",
    "  0.7713713268032057,\n",
    "  0.7693677649154052,\n",
    "  0.7615761353517364]]\n",
    "\n",
    "F1 = [[0.8721646270645552,\n",
    "  0.8454380830684984,\n",
    "  0.8308035556798741,\n",
    "  0.8427847087818908,\n",
    "  0.8361367786545584,\n",
    "  0.800560150956417,\n",
    "  0.8959551027723758,\n",
    "  0.8850686674618192,\n",
    "  0.8829398976340694],\n",
    " [0.769528754611683,\n",
    "  0.7672068184702937,\n",
    "  0.756352812452187,\n",
    "  0.7456473586120339,\n",
    "  0.759825550646379,\n",
    "  0.7456473586120339,\n",
    "  0.8134960144432843,\n",
    "  0.8113607490231142,\n",
    "  0.7958470832828048],\n",
    " [0.733008775633781,\n",
    "  0.7513925872900818,\n",
    "  0.7102989560671727,\n",
    "  0.7029327262848254,\n",
    "  0.7337654353340679,\n",
    "  0.7029327262848254,\n",
    "  0.7779240262969609,\n",
    "  0.7702834181445732,\n",
    "  0.7590528214009492]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "949d59e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x18e641810>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = plt.cm.tab20c(np.linspace(0, 0.54, 9))\n",
    "ysr=['First Label', 'Second Label', 'Third Label']\n",
    "x1 = [1,11,21] # x轴点效率位置\n",
    "x2 = [i + 1 for i in x1],
    "x3 = [i + 2 for i in x1],
    "x4 = [i + 3 for i in x1],
    "x5 = [i + 4 for i in x1],
    "x6 = [i + 5 for i in x1],
    "x7 = [i + 6 for i in x1],
    "x8 = [i + 7 for i in x1],
    "x9 = [i + 8 for i in x1],
    "\n",
    "y1 = [i[0] for i in Acc],
    "y2 = [i[1] for i in Acc],
    "y3 = [i[2] for i in Acc],
    "y4 = [i[3] for i in Acc],
    "y5 = [i[4] for i in Acc],
    "y6 = [i[5] for i in Acc],
    "y7 = [i[6] for i in Acc],
    "y8 = [i[7] for i in Acc],
    "y9 = [i[8] for i in Acc],
    "ax1 = plt.subplot(1,1,1)\n",
    "plt.sca(ax1)\n",
    "plt.xlabel(\"\") #X轴标签\n",
    "plt.ylabel(\"T est Accuracy\")  #Y轴标签\n",
    "plt.bar(x1, y1, alpha=0.7, width=1, label=\"Bag Vector + Naive Bayes\", color = color[0])\n",
    "plt.bar(x2, y2, alpha=0.7, width=1, label=\"Bag Vector + Random Forest\", color = color[1])\n",
    "plt.bar(x3, y3, alpha=0.7, width=1, label=\"Bag Vector + SVM\", color = color[2])\n",
    "plt.bar(x4, y4, alpha=0.7, width=1, label=\"Tf-idf + Naive Bayes\", color = color[3])\n",
    "plt.bar(x5, y5, alpha=0.7, width=1, label=\"Tf-idf + Random Forest\", color = color[4])\n",
    "plt.bar(x6, y6, alpha=0.7, width=1, label=\"Tf-idf + SVM\", color = color[5])\n",
    "plt.bar(x7, y7, alpha=0.7, width=1, label=\"FastText\", color = color[6])\n",
    "plt.bar(x8, y8, alpha=0.7, width=1, label=\"TextCNN\", color = color[7])\n",
    "plt.bar(x9, y9, alpha=0.7, width=1, label=\"TextCNN + Word2Vec\", color = color[8])\n",
    "ax1.set_xticks(x3)\n",
    "ax1.set_xticklabels(ysr)\n",
    "box = ax1.get_position()\n",
    "ax1.set_position([box.x0, box.y0, box.width* 0.8 , box.height])\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5),ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "771fe736",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x16c008f10>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfcAAAD4CAYAAADxVK9GAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAABDlUlEQVR4nO3deVxUVf8H8M93hl0QAWWRRVAZhwFEEyUMMy1NSs1S3BLNUkIfQ7MyHzUtnxb9lVn2pGJmRqZmarmWZSpZbuECCgKiggsoAsoi6zDn98fM+IwIw6Cs4/f9evFy5p5zzz1z4eV3zrn3ni8JIcAYY4wx4yFp6g4wxhhjrH5xcGeMMcaMDAd3xhhjzMhwcGeMMcaMDAd3xhhjzMiYNHUH6qpt27bC09OzqbvBGGsBjh8/niOEaNfU/WCssbW44O7p6Ym4uLim7gZjrAUgooym7gNjTYGn5RljjDEjw8GdMcYYMzIc3BljjDEjw8GdMcYYMzIc3BljjDEjw8GdMcYYMzIc3BljjDEjw8GdMcYYMzIc3BljjDEj0+JWqHuYTd+/SG/55/1mN1JPGGOMNWc8cmeMMcaMDI/cjciapLW11nlZ8VKD94MxxljT4uD+kNl3dY/e8v6uTzdSTxhjjDUUnpZnjDHGjIzRj9ynb4yvtc7nowMaoSeMMcZY4+CRO2OMMWZkOLgzxhhjRoaDO2OMMWZkOLgzxhhjRoaDO2OMMWZkjP5ueUOsPHxZb3lksHsj9YQxxhh7cBzc60neopF6y+1nb2qknjDGGHvY8bQ8Y4wxZmR45N5IVD+/rbdcMmxxI/WEMcaYsePgboC9qdm11nmkEfrBGGOMGYKn5RljjDEjw8GdMcYYMzIc3BljjDEj06DX3IloEIDPAUgBrBZCLKpSbgtgHQAPTV8+EUJ805B9Ykxr+v5FessDnJxrbeNlxUv11BvGGKs/DTZyJyIpgC8BhAJQABhDRIoq1f4FIEkIEQDgCQBLiMisofrEGGOMPQwaclq+F4A0IcQFIUQ5gI0AnqtSRwCwISICYA0gD4CyAfvEGGOMGb2GnJZ3BaC7rusVAEFV6vwXwHYAmQBsAIwSQqiqNkREEQAiAMDDw6NBOtvUVIdXNnUXGGOMGYmGDO5UzTZR5f3TAE4B6A+gE4DfieigEKLgrp2EWAVgFQAEBgZWbYOxJrPv6h695f1dn26knjDG2P805LT8FQC6GVfcoB6h65oIYKtQSwNwEYC8AfvEGGOMGb2GDO7/APAmIi/NTXKjoZ6C13UJwJMAQEROALoAuNCAfWKMMcaMXoNNywshlEQ0DcAeqB+FWyOESCSiSE35SgD/AbCWiE5DPY3/thAip6H6xBhjjD0MGvQ5dyHEbgC7q2xbqfM6E8DAhuwDY6xx1LZuwOf9ZjdSTxhjvEIdY4wxZmQ4Kxyrk+kb42ut49PBXm95ZLC73nJmnNYkrdVb3lir/R0/ftzRxMRkNQA/8ACHtVwqAGeUSuWkHj163JO6lIM7Y+yhYmJistrZ2dmnXbt2NyUSCT9ay1oklUpFN27cUFy7dm01gKFVyzm4sxZJ9fPbesslwxY3Uk9YfaltzQCg3tYN8OPAzlo6iUQi2rVrl3/t2jW/6so5uLNGtzf1nhmkezwlc2yEnrCHlIQDOzMGmr/jai8tcXBnrJlbefiy3nK+h4ExVhUHd9Ys5S0aqbe8jdxLbzmv1c8MFRFzonN9trdq/CNptdWRSqU9vL29S4QQkEql4vPPP780YMCA2w9y3J07d9rMmzfP9dSpU8nabRUVFXB2dg44ceJEUocOHSoMbWvhwoWOr7/+eo6Njc09uT4aQkpKiplcLvd///33L8+dOzcbAMaPH+8RGBh4OyoqKrem/f7v//6vnZWVlWratGk11jHEzJkz269bt66tvb29sqysjHr37l0YExNzSSqVPkizTYrvFGWMsUZmbm6uSk5OTkpJSUn6z3/+c3XOnDluD9pmaGho4bVr18xSUlLupM3etm1ba5lMVlKXwA4A0dHRTkVFRXWKD0plzQk9XV1d/Wvb397eXhkdHe1YWlpaXV6Sas2aNevGgwZ2rcjIyOvJyclJaWlpicnJyZa7d++2qY92mwoHd8YYa0L5+flSW1tbpea1JDg4WKZQKHxkMpli3bp1bbT13nrrLRcvLy/f3r17ew8ZMsRr/vz5TrrtSKVSDB48OC8mJubOs6gbNmywDwsLyysoKJCEhYV5+vn5+fj4+NxpV6lUIiIiwk0mkylkMpnigw8+cHz//fcds7OzTfv27SsLCgqSAUB0dLS9TCZTeHt7+06ZMsVV276VlVX3GTNmtO/atav8jz/+sH6Q82Bvb68MCQkp/PLLLx2qli1ZsqStn5+fT5cuXRRPP/10p8LCQgmgHnHPnz/f6cSJExb+/v4+2vopKSlmMplMAQAHDx606tmzZxdfX1+fkJAQ74yMDFN9/SgrK6OysjKJg4ODsqZj37x5U+Lq6upfVlZGAJCXl3fnfWJionmfPn28fX19fXr06NHl5MmTFgCwZs0aO29vb98uXbooAgMDuzzIuTIET8sz1oTqY92A+sBPHzSusrIyiVwuV5SVlVFOTo7p7t27UwHAyspKtWvXrjR7e3tVVlaWSVBQkHzs2LG3/vrrL6sdO3bYnT59OqmiooK6deum6N69e3HVdsPDw/MiIyM9P/jgg2slJSW0f/9+25UrV16eM2eOS79+/Qp+/PHH9JycHGlgYKDP0KFDC1asWOGQkZFhnpiYmGRqaorr169LnZycKlesWOEUGxub6uLiokxPTzd99913XY8fP362Xbt2yj59+si+++67NuHh4bdKSkokfn5+JZ999lnVpGD3Zf78+VmhoaHe06dPv2sZ8hdffPHmG2+8kQMAUVFR7ZctW9ZWO30PAI888khpRUUFJSUlmSkUivKYmBj7YcOG3SwrK6OoqCiPXbt2pbVv31751Vdf2b355puuP/74Y3rVY69cudJp06ZNDpmZmWZ9+/bN7927d4m+YwcHBxdu2rTJNjw8/NaaNWvsn3nmmZvm5uZi0qRJHVatWpXh7+9ftm/fvlZTpkzxOHLkSOqiRYtcfvvtt1QvL6+KnJycBp/v55E7Y4w1Mu20/MWLFxN/+umncxMnTvRSqVRQqVQ0Y8YMN5lMpujXr58sOzvb7MqVKyYHDhywDg0NvWVtbS3s7OxUAwYMuFVdu3379i0uLi6WxMfHm2/evNm2W7dut9u1a1d54MCB1kuXLnWRy+WKkJCQLmVlZZSWlma2b9++1pGRkTdMTdWDWScnp8qqbf7111+tHn300cL27dsrTU1NMWrUqLzY2FhrQD1b8NJLL92sri9vv/22s1wuV8jlckV2drap9nV4eLhHTedFLpeXd+vW7XZ0dPRd32iPHz9u2aNHjy4ymUyxZcsWh8TERIuq+w4bNixv3bp19gDw008/2YWHh+clJCSYnzt3zrJ///4yuVyu+Pjjj10yMzOrHblrp+Vv3LgRX1xcLFm1apWdvmNHRETcWLt2rQMArFu3rm1EREROfn6+5OTJk9ZhYWGd5HK5YurUqR2ys7NNASAwMLDoxRdf9FyyZElbfZcw6guP3BljrAk99dRTt2/evGmSlZVlsmXLFtvc3FyT06dPnzU3Nxeurq7+JSUlEiEMf3Jv2LBheTExMfYpKSmWo0aNygMAIQQ2b96cFhAQUKZbVwgBItLbuL5jm5mZqUxMqg8jixcvvrZ48eJrgPqae3JycpIh/Z8/f/61kSNHdgoKCirUbouIiPDavHlzWnBwcMmyZcscYmNj77keHh4efjMsLKzj6NGjbxIR/P39y44dO2bZuXPnEt2bDGtjbm4uBg4cWPDnn3/aRERE3Kzp2AMHDrz92muvme/atcu6srKSevbsWZqXlyexsbFRVvdZ169ff2nfvn2ttm/fbtutWzffU6dOJTo7O9/zZaq+8MidMcaa0MmTJy1UKhWcnJyU+fn50rZt21aYm5uLHTt22GRmZpoBwBNPPFG0Z88e2+LiYsrPz5fs3bu3TU3tjR8/Pm/z5s0Ohw4dshkzZswtAOjXr1/BkiVLnFQq9c3vf//9tyUAPPXUUwUrV65sV1Ghvt/u+vXrUgBo1apVZX5+vgQAHn/88dtHjx61ycrKMlEqlfjxxx/tn3jiiaKGOh/du3cv9fb2Lvnjjz9stduKi4slHh4eFWVlZbRx48Zqr1P5+vqWSSQSzJ8/v/3zzz+fBwBdu3YtzcvLM9m7d28rQH09PS4u7p5Rvy6VSoXDhw9bd+rUqay2Y48ePTp34sSJHceNG5cDAPb29io3N7fyNWvW2Om0ZQkAiYmJ5v3797/92WefZdrZ2SkvXLhgVvXY9YlH7oyxh5ohj67VN+01d0A9Ml6xYkW6iYkJJk2alBcaGtrZz8/Px9fXt9jLy6sUUE+3Dxo0KF+hUPi6urqWde3a9batrW21o74ePXqUWlhYqPz9/Ytbt26tAoBFixZlRkREeMjlcoUQgtzc3Mr279+f9vrrr99ITU01l8vlviYmJmLChAk35syZc2PChAk5oaGh3o6OjhVHjx5NnT9//tW+ffvKhBD05JNP5o8bN+5WQ56fd955J+uxxx5TaN/Pnj07s1evXj6urq7lPj4+xUVFRdVes37hhRfy/vOf/7gtXrz4KgBYWFiIjRs3no+KivIoLCyUVlZW0pQpU64HBgaWVt1Xe81dqVSSj49P8VtvvZVd27FfeeWV3MWLF7u+8soredptGzZsuDB58uQOixcvdlEqlfT888/nBQcHl7z++utu6enp5kIICgkJKXj00UdL6vOcVUV1me5pDgIDA0VcXJzB9evjhqXODua1tvHI1ml6y2t7LhtOtZQDeL30lt7yACfnWtvwtHXRW17b8p58Pu/WHM5nfSxiUx831NWW8rW281nbuQTqvvwsER0XQgTqbouPj08PCAjIqWmf5io/P19ia2urKiwslAQHB3dZuXJlRkhIyD031bHG880339ht27atzc8//3yxqfoQHx/fNiAgwLPqdh65M9bC1bacLy/laxzGjRvX4dy5c5ZlZWU0evToXA7sTWvChAnu+/fvt925c+e5pu5LdTi4M2bkalvtD+AV/1qCHTt2NNnokN3r22+/vQxA/9rQTYhvqGOMMcaMDAd3xhhjzMhwcGeMMcaMDAd3xhhjzMjwDXWMsYfa5wcz6jXl6/Q+HTjlax2lpKSYBQQE+Hl6epZWVFRQ165db2/cuDHD3Nz8gZ/VnjlzZntra+vKhQsXXq+Pvmq5urr6t2rVqlIiUY+Rv/jii4wH/R1W59ChQ5aXL182GzVqVH5d9uORO2OMNTJO+Xovd3f3Ms05SczKyjLTrvLWnMXGxqYmJycnJScnJxka2LWrARoqLi7OateuXba117wbB3fGGGtCnPL1biYmJnjkkUduX7161RQA1q9fb9u1a1e5j4+Ponfv3rLLly+bAOoReVhYmGevXr26uLm5+b///vt3FnR4++23nT09Pf169+4tO3fu3J1Vsw4dOmQZEBAgl8lkigEDBnS6ceOGFAB69erV5ZVXXnEPDAzs0rFjR9/Y2FirgQMHdurQoYNfVFRUe0P7npqaahYcHCyTyWSK4OBg2blz58wAYPjw4Z6TJk1yCwoKkk2dOtXN0LSwpaWl9NFHH7XfsWOHnVwuV3z11VcGf+HhaXnGGGtknPK1ZsXFxXT8+PFWy5YtuwwAAwYMKBo9enSyRCLBp59+2nbhwoXOX3311RUASEtLszh06FDKrVu3pD4+Pn5vvfXWjWPHjln+9NNP9ppzBd1z9dJLL3ktXbr00rPPPls0Y8aM9m+//Xb7NWvWXAbUSXDi4uJS/vOf/ziGhYV1/ueff846OjoqPT09/efMmXO9uiQvffv2lUkkEpiZmakSEhKSIyMjPcaOHZv72muv5X722WcOU6ZMcd+7d+95ADh//rzF33//nWpiYoLg4GCZIWlhLSwsxL///e/MuLi4VjExMZfqch45uDPGWCPTTssDwN69e1tNnDjRKzU1NVGb8vXIkSPWEokE1aV8BSAMSfmakJBgqZvydc+ePW2WLVvmDKgTqNxPylcAd1K+hoeH36ot5eu2bdvsAUCb8hUAevbsWfTdd9/dE6guX75sLpfLFRkZGeahoaE3g4KCSgDg4sWLZsOGDXO7ceOGaXl5ucTd3f1OZruBAwfesrS0FJaWlkp7e/uKK1eumOzfv9/6mWeeuaW9X2DgwIG3ACA3N1daWFgoffbZZ4sAYPLkyblhYWEdtW09//zztwAgICCgpHPnzncuZbi7u5dduHDBzNnZ+Z614LVfgLTvT5482eqXX345DwBTpkzJe++99+5cbnnhhRdumpiYQDctrLasvLycgP+lhR0+fPjNF198sdrzaigO7owx1oQ45aua9pp7RkaGad++fbt8//33ti+++GL+tGnTPKZPn37txRdfzN+5c6fNwoUL70yT695wJ5VKoVQqCQCISN+hqmVhYSEAQCKR3NWuRCK50+6DsLa2VgFAZWUl6pIW9n6Px9fcGWOsCXHK17t16NChYuHChVc+/vhjFwAoLCyUenh4VADA2rVrHWrbv3///kW7du1qU1RURDdv3pT8/vvvbQDAwcGhsnXr1pW//vqrNQB8/fXXDsHBwfX6Obp373579erVdoD6PoXAwMB72q9rWtjWrVtX1vXmRoBH7oyxh5whj67VN075qt+4ceNuffDBB+1//fVX67lz52aOGTOmk5OTU3lgYODtS5cu6U0rGRISUvz888/n+fn5+bq6upb16tXrToD95ptvLk6ZMqVDVFSUxMPDo2zDhg3p9dnvFStWXJowYYLn559/7uzg4KCMiYmptv26pIXt1KlT+SeffOIil8sVb7zxRtbkyZMNmq7nlK/gFKW6OOXr/xjL+aztXALN43xyylf9OOUrqw6nfGWMsRaMU76yuuDgzhhjLQCnfGV1wTfUMcYYY0aGgztjjDFmZDi4M8YYY0aGgztjjDFmZAy6oY6IQgB4CyG+IaJ2AKyFEHxzB2Osxfsl+Xq9pnwNlTtxytc6qqysxCuvvOL+999/tyYiYWZmJjZv3nx+7ty57R999NGit956686ji999912b1atXt42NjU0joh7PPfdc3s8//3wRUH9eR0fHgG7dut3ev39/o69f0JzUOnInogUA3gbwb80mUwDrDGmciAYRUQoRpRHR7BrqPEFEp4gokYhiDe04Y4y1VJzy9W6rV6+2v3btmmlycnJiampq0rZt29IcHBwqx44dm7d58+a7Fnr44Ycf7EeOHJkHAJaWlqqUlBTLoqIiAoCffvqptZOTU91yqhopQ355zwMYCuA2AAghMgHY1LYTEUkBfAkgFIACwBgiUlSp0wbAcgBDhRC+AMLq0nnGGGvpOOUrkJWVZerk5FQhlUoBAJ06dapo165d5XPPPVdw4cIFi4yMDFMAKCwslPz99982Y8eOvaXd98knn8z/8ccf22g/7/Dhw/Putx/GxJDgXi7Uy9gJACCiVga23QtAmhDighCiHMBGAM9VqTMWwFYhxCUAEEJkG9g2Y4y1WNrlZ728vHynT5/eYcGCBVnA/1K+JiUlnY2NjU2dM2eOm0qlwp9//nkn5euuXbvOJyQkVPv/cHh4eN7WrVvtAUCb8jU8PPymNuXrmTNnzh48eDBl3rx5bgUFBZIlS5a006Z8TU1NTZo0aVLuvHnzsh0dHStiY2NTjx49mqpN+XrgwIHUpKSkxJMnT7b67rvv2miOIfHz8ytJSEhIfvrpp+97nfbw8PC8vXv3tpHL5YrJkye7ade+NzExwaBBg27FxMTYAcCGDRtsH3300UI7OzuV7r4//PCDXXFxMZ09e9YqODj4gS5vGAtDgvsmIooG0IaIJgPYC+ArA/ZzBXBZ5/0VzTZdMgB2RHSAiI4T0fjqGiKiCCKKI6K4GzduGHBoxhhrvrTT8hcvXkz86aefzk2cONFLpVJBm/JVJpMp+vXrJ6su5audnZ3KkJSvmzdvttVN+bp06VIXuVyuCAkJ6XK/KV9NTU3vpHwF1LMF+lK+yuVyhVwuV2hTvsrlckV4eLhH1bqdOnWqSEtLO7Nw4cIrEokEzzzzTJdt27bZAMC4cePytmzZYg8AmzZtsh89evRdI/OgoKCSK1eumH/11Vf2Tz31VH7dfhPGS+8NdaTOm/cDADmAAgBdAMwXQvxuQNvVpcirupC9CYAeAJ4EYAngMBEdEUKk3rWTEKsArALUa8sbcGzGGGsROOWrmqWlpRg5cmTByJEjC5ycnCq2bt3a5rnnniscMGBA0csvv2x6+PBhyxMnTlhv3779QtV9Bw0adGvBggXuv/32W0p2djavvIpaRu6a6fifhRC/CyHeEkK8aWBgB9QjdXed924AMqup86sQ4rYQIgfAnwACDGyfMcZaPE75Cvz1119W6enppoD6zvnTp09bdujQoRxQ51MfMmTIzYkTJ3r169cv38rK6p5vG1OmTMl54403Mnv16lVSn/1qyQz5hnOEiHoKIf6pY9v/APAmIi8AVwGMhvoau65tAP5LRCYAzAAEAVhax+Mwxth9M+TRtfrGKV/vdu3aNZNXX321Q3l5uQQAunXrdnv27Nl37sEKDw/PjY6Odnr//fevVLd/p06dKt555x2+Z0uHIcG9H4BIIkqH+o55gnpQ31XfTkIIJRFNA7AHgBTAGiFEIhFFaspXCiHOEtGvABIAqACsFkKcuf+PwxhjzV9lZeXx6ra7uLgodZ9T17VgwYJrn376aaY25eusWbOu19R+SkrKXVPg1tbWYv369RlV65mammL16tVXoJ5FvWPu3LnZc+fOvRMsIyMj8yIjI++5C724uPhkTX3QdfXq1dP6ykeMGFEwYsSIgprKH3vssRIhxD3nrLrjDx48uHDw4MGFhvTLmBkS3EPvt3EhxG4Au6tsW1nl/ccAPr7fYzDG2MOAU76yuqg1uAshMogoAEAfzaaDQoj4hu0WY4wxXZzyldWFISvUTQfwPQBHzc86InqtoTvGGGOMsftjyLT8KwCChBC3AYCIFgM4DOCLhuwYY4wxxu6PIYvYEADduzIrUf0z7IwxxhhrBgwZuX8D4CgR/aR5PwzA1w3WI8YYY4w9EENuqPuUiA4ACIF6xD5RCGHQ4w+MMdbc5b73bL2mfHVYsEvvc/PXrl2TPvHEE10AICcnx1QikQh7e3slAIwbNy5nzZo17fz8/Iq3b99+5wa6P//802rNmjUOa9euvVy1PVdXV/+4uLizLi4uyvfff9+xuv3rqlevXl2Ki4slZ86cOas9/ptvvul+7NixlJr2SU9PN42MjHT/9ddf71lBri5SUlLMAgIC/Dw9PUuFELCyslKtXbv2YtXV9Zh+tQZ3InoUQKIQ4oTmvQ0RBQkhjjZ47xhjzMg4OztXapdinTlzZntra+vKhQsXXgcALy8v319++eWcXC4v193n8ccfL3788cdrffTt66+/blfd/rpmzpzZ3tPTsywqKipXX1u5ubkmmzZtaj1y5Mganz/X5enpWfGggV3L3d29THuOPv7447bvvfeey9atW9Pro+2HhSHX3FcA0F1q8LZmG2OMsXoyduxYjytXrpgPHTq083vvveeoW7Zz506bfv36dQbUI//HHnvM28fHRzF27NgO2rXf9e1/P6ZNm3Z90aJF7atuT0lJMevRo0cXhULho1AofH7//fdW2u3e3t6+ANC1a1d5XFychXafXr16dTl48KBVTaln9SkoKJC2adOmUt+xhw0b5qXb1tChQ72+//57W6VSiVdffdXNz8/PRyaTKT7++OO2AJCRkWEaGBjYRS6XK7y9vX1//fXX+05X21wZcs2dhE7mACGESrNcLGOMsXqyfv36S7GxsbaxsbGpLi4uyprqzZ49u31wcHDRJ598krVx40bbDRs2tK3L/obq06dP0Y4dO9rs2LHDRnep2/bt2ysPHjyYamVlJU6fPm0+ZsyYjtrpe63hw4fnff/99/aBgYGZGRkZptnZ2aZ9+vQpnjZtmmu/fv0Kfvzxx/ScnBxpYGCgz9ChQwu0y+RqXb582Vwulytu374tKS0tlRw6dChZ37EnT558Y+nSpU7jxo27lZubKz1+/Lj1li1bLn722WdtbW1tK8+cOXO2pKSEevbsKR8yZEjBhg0b7J588sn8xYsXX1MqlSgsLDRkoNuiGPKBLhBRFBGZan6mA6iXqRfGGGN1c+TIEZuXX345FwBGjx6d37p162rXmNd17NgxS23K1ZiYmHYfffRRe+37a9euSWvab86cOVkffvihi+628vJyGjt2rKdMJlOEhYV1On/+vEXV/caPH39z+/btdgAQExNjN2TIkJsAUFPq2ar7a6flL1++fObDDz+8/PLLL3fQd+xnn322KCMjw+Lq1asmX3/9tf2zzz5709TUFHv37m29adMmB7lcrujevbvPzZs3TZKSkiweffTR2xs2bGg7c+bM9seOHbPUzQ9vLAwZgUcCWAZgnub9XgARDdYjxhh7yMXExLT58MMP2wPAqlWr0quWSyR1G2j26tWrRPc6vyHX3AFg6NChhe+++67rX3/91Uq77YMPPnBydHSs2LJly0WVSgVLS8seVffz8vKqaNOmjfLo0aOWW7dutY+Ojs4Aak49q8+YMWNuRUVFedZ27JEjR+auXr3afsuWLfZr1qxJ1xyPlixZcmn48OH33Dfw559/pmzZssX2pZde8oqKiro+bdq0Ws9HS1LrX4gQIlsIMVoI4aj5GSuE4Ow7jDHWQMaPH38rOTk5KTk5OanqjXSPPvpo4Zo1axwAYNOmTa0LCgpqHHnXh7fffjvriy++cNa+z8/Pl7q4uFRIpVIsX77cobKy+omDESNG5H344YfOhYWFUm0q1ppSz+rz+++/27i7u5fVduzIyMic6OhoJwAIDAwsBYABAwbkr1ixol1ZWRkBQEJCgnlBQYEkNTXVzNXVteKNN97IGTduXM6JEyes7vP0NFs1jtyJaDKAA0KIc0REUD/bPhxABoCXtHfPM8ZYS1bbo2vNzaJFizKHDx/eUaFQ+AQHBxe5uLjUeGd8fRg1alT+woUL71zDnzFjRvbw4cM7/fzzz3YhISGFlpaW1U5pjxs37uY777zjMX369EzdvleXerbqvtpr7kIImJqaipUrV2bUdmx3d3dlp06dSocMGXJLu+3111/PSU9PN/f39/cRQpC9vX3F7t27z+/Zs8dm2bJlziYmJsLKyqry+++/N7p1+0nnXrm7C4jOAOguhKggorEA3gAwEEB3AAuEEH2q3bGBBQYGiri4OIPrT99Ye44bnw72ess7O5jX2sYjW6fpLW8j99LfgFMt5QBeL72ltzzAyVlvOQB42rroLe/v+rTecj6fd2sJ57O2cwk0j/NZ27kEaj+fVRHRcSFEoO62+Pj49ICAgJw6NcSavcLCQolCoVCcOnXqrIODQ633IRiL+Pj4tgEBAZ5Vt+ubllcKISo0rwcDiBFC5Aoh9gJopWc/xhhjrNH8/PPPNjKZzHfy5MnZD1Ng10ffDXUqInIBcBPAkwA+0Cmr9ToJY4wx1hiGDRtWOGzYsNNN3Y/mRF9wnw8gDoAUwHYhRCIAEFFf8KNwjDHGWLNVY3AXQuwkog4AbIQQN3WK4gCMavCeMcYYY+y+6H0UTgih1A3sRLRKCHFbCFGkbz/GGGOMNZ26LrkXWHsVxhhjjDWluq4Rz4vXMMaMiurHqHpN+SoJW2YUKV+zs7NNzc3NVaampmLVqlXpvXv3Lrnf9rRSUlLMBg8e7H3u3LnEB21L18yZM9uvW7eurfY89u/fP3/58uVX6/MYWrNnz3ZetGjRtYZouz7VKbgLIQY1VEcYY+xh0FJSvsbExFx4/PHHiz///HOHN9980+3QoUPnDPuETSMyMvK69jwaSqlUwsSkbmPcZcuWubSE4H5fmXCIaFV9d4Qxxh5mzS3lq9bjjz9++/r162YAkJ+fLwkODpYpFAofmUx2J2VrSkqKWceOHX1Hjx7doXPnzr6PPfaYd1FREQHAwYMHrbp06aLo1q2b/NNPP73Tr+LiYhoxYoSnTCZT+Pj4KHbs2GEDAMuWLXN46qmnOvXv37+zq6ur/4cfftju3XffdfLx8VEEBATIr1+/btByuyqVCq+++qqbt7e3r0wmU3z11Vd22nMZFBQkGzJkiFeXLl1865IWdurUqa5lZWUSuVyuGDp0aO0rOzWhGoM7EdnX8OMA4JlG7CNjjBm99evXX3J0dKyIjY1NXbBgQY2XQLUpX8+ePZs0dOjQW1lZWWZ12b+uduzY0To0NPQWAFhZWal27dqVlpSUdDY2NjZ1zpw5btp14i9dumQRFRWVnZaWlmhra1sZExNjBwCvvPKK56effnrp1KlTybrtLl682BEAUlNTk9avX38hIiLCs7i4mDTbLLds2XLhn3/+OfvRRx+5WllZqc6ePZsUGBh4Ozo62qG6fq5cudJJm+luy5YtrWNiYtqcPn3a8uzZs4l//PFH6vz5890yMjJMASAhIaHVxx9/fPX8+fOJumlh4+Pjz3777bftkpOTzdasWWP/5JNP5icnJyedPXs2MSgoqHj58uVXzc3NVcnJyUkPctmjMeibj7gB9TrypLNNaN7X27dCxhhjhjty5IjN1q1b0wB1ytdXX33VoJSv48eP9wLU1/lNTU1Vy5cvdwKAAwcOpDg7O9/Txvjx4zuWlJRIVCoV4uLizgKASqWiGTNmuB05csRaIpEgOzvb7MqVKyYA4OrqWqa9Lt+9e/fi9PR089zcXGlhYaH02WefLQKAl19+OXffvn22AHDo0CHr1157LVtTv7R9+/blp0+ftgCA3r17F9rZ2ans7OxU1tbWlWFhYbcAwN/fvzghIaHaJC9Vp+VfeeUV95EjR+aZmJjA3d1dGRQUVPTXX39Z2draqrp27Xpbe+li7969rZOTk620KWoLCwul2rSwr776qmdFRYVkxIgRN+vjnoPGpC+4XwDwpBDiUtUCIrrnpg7GGGP1ozmkfI2JibkQFBRUMm3aNNfJkyd7/Pbbb+ejo6Ptc3NzTU6fPn3W3NxcuLq6+peUlEgAwMzM7E6iEqlUKkpKSiRCCKjzjt2rprwmVduSSCSwsLAQ2tdKpbL6BuvQvpWVlUqnnlGmhdX3F/IZALsayv6v/rvCGGMMaD4pX83NzcXSpUuvnjp1qtWJEycs8vPzpW3btq0wNzcXO3bssMnMzDTTt3/btm0rra2tK/fs2WMNAGvXrr2TBSkkJKRo3bp19oA6FWtWVpZZ165dS+ur73379i3cvHmzvVKpRGZmpsmxY8es+/Tpc7tqvbqmhTUxMRHaus2ZvhXqvtRT9kXDdIcxxhpXbY+uNTeNnfLV2tpaTJky5fqiRYucPv/886uhoaGd/fz8fHx9fYu9vLxqDcZff/11+qRJkzwtLS1V/fv3vzM6njVrVnZ4eHgHmUymkEqliI6OTre0tKx5uF1H4eHhtw4dOmTt4+PjS0Tivffeu+Lh4aFMSEi4q15d08K++OKLN3x8fBQP+rhhQ9OX8vVDIcQczesBQojfG7VnNeCUrzUzlhSlAJ9PXZzy9X845Stjd7uflK+6z7QvrvceMcYYY6xB3Ndz7owxxhhrvvTdLe9IRDOhefRN8/oOIcSnDdozxhhjjN0XfcH9KwA21bxmjDHGWDOm72759xqzI4wxxhirH3zNnTHGGDMydU35yhhjRkV18PP6TfnaZ3qLT/m6YcMG24ULF7qqVCoolUqKjIy87uPjUzZv3jxX3TXiKyoq4OzsHHDixImkmTNnuu7evdsuMzMz3s7OTgUAEydOdF+7dq1jZmZmvIuLi/J++8PqrtbgTkReQoiLtW1jjDFWu+ae8rWsrIymT5/e4fDhw2c7depUUVJSQqmpqWZ+fn5lkyZNMktJSTHr0qVLOQBs27attUwmK+nQoUMFALi7u5dt2LChzdSpU/MqKyvx999/2zg6OlYYfnZYfTFkWn5LNds2G9I4EQ0iohQiSiOi2Xrq9SSiSiIaYUi7jDFmbJpLytdbt25JlEolOTk5KQHA0tJSBAQElEmlUgwePDgvJibmzqpKGzZssA8LC8vTvh8+fHje5s2b7QFg165dNj179iwyMTGpt1XnmOH0pXyVE9FwALZE9ILOz0sALGprmIikAL4EEApAAWAMESlqqLcYwJ77/AyMMdbiNZeUr05OTpUDBgy45eHh0XXIkCFeK1assK+sVCeNCw8Pz9u6das9AJSUlND+/fttw8PDb2r3lclkZbm5uSY3btyQrl+/3n7s2LF5NRyGNTB90/JdAAwG0AbAEJ3thQAmG9B2LwBpQogLAEBEGwE8ByCpSr3XoJ4d6GlYlxlj7OHVGClff/jhh4xjx45l//LLLzbLli1z3rt3b+stW7ak9+3bt7i4uFgSHx9vnpCQYNmtW7fb7dq1u2vfIUOG3FyzZo39iRMnWn3//fcZ9ffJWV3oexRuG4BtRBQshDh8H227AtC9+eMKgCDdCkTkCuB5AP2hJ7gTUQSACADw8PC4j64wxljL0RxSvvbq1aukV69eJREREXmdO3f2B5AOAMOGDcuLiYmxT0lJsRw1atQ9I/MJEybc7NWrl8+IESNypdIGS1jHamHIX8jzRNSaiEyJ6A8iyiGicQbsV11KvKrXXj4D8LYQQu83TyHEKiFEoBAisF27dgYcmjHGWq6mTPman58v2blz551Fy44ePWrZvn37OzfojR8/Pm/z5s0Ohw4dshkzZsytqvt7e3uXz5kz5+qMGTNu1Ge/WN0Y8ijcQCHELCJ6HurRdxiA/QDW1bLfFQDuOu/dAGRWqRMIYCMRAUBbAM8QkVII8bMB/WKMsQdW26NrzU1Dp3xVqVT4+OOPnaZNm9bBwsJCZWVlpfr666/vPB3Vo0ePUgsLC5W/v39x69atVdW18dZbb3HWvSZmSHA31fz7DIANQog8TTCuzT8AvInIC8BVAKMBjNWtIIS4k0eSiNYC2MmBnTH2sPj000/vGvBcvXr1dHX1Bg8eXDh48OBCQP0o3d9//31Op/jO5c+a9td3zKrs7OxUsbGxer/wpKSkVL13Clu2bEmvrq4hfWL1z5DgvoOIkgGUAJhKRO0AlNa2kxBCSUTToL4LXgpgjRAikYgiNeUrH6DfjDHGGKtBrcFdCDGbiBYDKBBCVBJRMdR3vddKCLEbwO4q26oN6kKIlwxpkzHGGGP61XpDHRFZAfgXgBWaTe2hvlbOGGOMsWbIkLvlvwFQDqC35v0VAO83WI8YY4wx9kAMCe6dhBD/B6ACAIQQJaj+MTfGGGOMNQOGBPdyIrKE5hl1IuoEoKxBe8UYY4yx+2bI3fILAPwKwJ2IvgfwGICXGrJTjDHWWCJ+f7deU76uGvBurc/NS6XSHt7e3iXa99u2bUvTZlozxMKFCx1ff/31HBsbG1XXrl3l5eXlkvz8fGlpaanEycmpoq5t6rZnaB9Y82bI3fK/E9EJAI9CPR0/XQjBCxQwxth9Mjc3V2mXg70f0dHRTpMnT86zsbFRJSQkJAPAsmXLHOLi4lrFxMRcepD27rdPrHkxZOQOIUQugF0N3BfGGHso5efnSwYNGtQ5Pz9fqlQqaf78+Znjxo27VVBQIBk6dGjHrKwsM5VKRbNmzcq8fv26aXZ2tmnfvn1ldnZ2yqNHj6ZWbS8xMdE8MjLSIy8vz8TCwkK1evXqDD8/v9Lu3bv7LFq06MrgwYML//Wvf7lKJBI4OTlV1NYea3lqDO5EZCKEUDZmZxhj7GFQVlYmkcvlCgBwd3cv27179/ldu3al2dvbq7KyskyCgoLkY8eOvbV169bWzs7OFQcOHEgDgNzcXKmDg0PlihUrnGJjY1NdXFyq/T960qRJHVatWpXh7+9ftm/fvlZTpkzxOHLkSOratWsvjhw5slNFRcXlffv22Z48efKshYWFqK091vLoG7kfA/BIY3WEMcYeFlWn5cvKymjGjBluR44csZZIJMjOzja7cuWKySOPPFIyd+5c9ylTprg+99xz+YMGDSqqre38/HzJyZMnrcPCwjppt5WXlxMABAYGlo4cOTJ39OjRnfft23fWwsKiajIvZiT0BXd+3I0xxhpBdHS0fW5ursnp06fPmpubC1dXV/+SkhJJ165dy06cOJG0ZcsW27lz57ru3bu34JNPPsnS11ZlZSVsbGyUNV3TT0xMtLSxsanMysoyhXpZcWaE9D0K146IZtb002g9ZIwxI5efny9t27Zthbm5udixY4dNZmamGQCkp6eb2tjYqKZOnZo3Y8aM66dOnbICgFatWlXm5+dX+/+3vb29ys3NrXzNmjV2gDrL2+HDhy0B4Ntvv22Tl5dnsm/fvuQ333zTIycnR1pbe6xl0jdylwKwBo/gGWNGzJBH1xrapEmT8kJDQzv7+fn5+Pr6Fnt5eZUCwPHjxy3//e9/u0kkEpiYmIjly5dnAMCECRNyQkNDvR0dHSuquwFuw4YNFyZPntxh8eLFLkqlkp5//vk8T0/PigULFrjt3bs3pXPnzhWTJk3KjoiIcN+6dWt6be2xlkdfcM8SQixstJ4wxthDori4+KTuexcXF+WpU6eSq9br0qVL+fDhw++ZXp87d2723Llzs3W3RUVF5QLIBQC5XF5+8ODBc1X3S09PP6N9PW/evDv7V9cea9n0TcPwiJ0xxhhrgfQF9ycbrReMMcYYqzc1BnchRF5jdoQxxhhj9YPvjmSMMcaMDAd3xhhjzMhwcGeMMcaMjEGJYxhjzFitPBNdrylfI/1e1fvc/LVr16RPPPFEFwDIyckxlUgkwt7eXgkAp06dMmhJ2NmzZzsvWrTomvb9pUuXTKZOneoRHx9vZWZmJtzc3Mq++OKLy+bm5kIul/u///77l7WPuo0fP94jMDDwdlRUVO7w4cM9Dx482DojI+O0paWlyMrKMgkMDPS5evXq6Qc7C6yp8cidMcYakbOzc2VycnJScnJy0vjx429ERkZe1743dK33ZcuWuWhfq1QqDB06tPPjjz9eePny5TPnz59P/Oijj65mZmaaAoC9vb0yOjrasbS0tNrHm6VSqVi2bFnb+vl0rLng4M4YY03s4MGDVj179uzi6+vrExIS4p2RkWGam5sr9fT09IuPjzcHgCFDhngtWbKk7dSpU121WeWGDh3qtXPnThsTExMxa9asG9r2evfuXaJNMmNvb68MCQkp/PLLLx2qO/arr76avWLFCqeKiorG+bCsUXBwZ4yxJiSEQFRUlMe2bdvOJyYmnp0wYULOm2++6erg4FC5dOnSSxMmTPBatWqV3a1bt0zeeOONnOXLl1/VZpXbvn37xYSEBMuAgIBifceYP39+1n//+18npfLejK4dOnQo79mzZ9Hy5curDf6sZeJr7owx1oTKysok586ds+zfv78MUE+zt2vXrgIAnn/++YJNmzbZzZo1q8Px48cT7/cYcrm8vFu3brejo6PtqytfsGBB1nPPPdd5xIgR+fd7DNa8cHBnjLEmJIRA586dS6pbW76yshKpqakW5ubmqpycHJNOnTrdM3fu7+9f8vPPP9vVdpz58+dfGzlyZKegoKDCqmV+fn5lCoWi+Ntvv621HdYy8LQ8Y4w1IXNzc1VeXp7J3r17WwFAWVkZxcXFWQDAwoULnWQyWem333574ZVXXvEsKysjADAxMRHa10OGDCksLy+nJUuW3LkpLjY21mrXrl3Wusfp3r17qbe3d8kff/xhW10/FixYkPXll186N9TnZI2LR+6MsYdabY+uNTSJRIKNGzeej4qK8igsLJRWVlbSlClTrpuZmYnvvvuu7fHjx8/a2dmpNm/eXDh79myXpUuXZr744os3fHx8FH5+fsXbt2+/uH379vNTp051/+yzz5zNzc3vPApX9VjvvPNO1mOPPaaorh+BgYGlvr6+xYmJiVYN/6lZQ+PgzhhjTeTTTz/N1L6Oi4tLqVp+4cKFO9fZV69efUX7esWKFVcBXNW+9/T0rNi9e/eF6o5x7ty5O20EBweXqFSq49r3W7ZsSdet+9tvv52v+6dgzRFPyzPGGGNGhoM7Y4wxZmQ4uDPGGGNGhoM7Y4wxZmQ4uDPGGGNGhoM7Y4wxZmT4UTjG2ENtz+Vd9Zry9Wn3Zx/qlK85OTnSzp07++fl5Z2SSCTYu3dvqwEDBsjT0tISOnXqVJGbmyvt2LGjf15e3impVFrn9nv16tXlk08+udy9e/fSIUOGdMzIyDCXSqUYOHDgreXLl1/duXOnzbx581x1V/yrqKiAs7NzwIkTJ5I6dOjwUGTI4ZE7Y4w1opae8nXnzp02w4cP96ypvG3btpVt27atOHnypAUAHDx40NrHx6d4//791gBw4MCBVgEBAbcNCey1Zap74403rl+8eDHxzJkzSUePHrXetGlT69DQ0MJr166ZpaSkmGnrbdu2rbVMJit5WAI7wMGdMcaanLGlfO3Zs2dRbGysNQAcOXLE+l//+tf1Q4cOWQPAX3/9ZR0UFFRUXFxMI0aM8JTJZAofHx/Fjh07bABg2bJlDqGhoR379+/fuU+fPrKioiIaPHhwR5lMpnj22Wc7ar+k2NjYqIYMGVIIABYWFqJr167Fly9fNpNKpRg8eHBeTEzMnSQ5GzZssA8LC8srKCiQhIWFefr5+fn4+Pgo1q1b1wYAlEolIiIi3GQymUImkyk++OADx3o7GU2kQYM7EQ0iohQiSiOi2dWUv0hECZqfQ0QU0JD9YYyx5sYYU74GBwcXHT582BoALl26ZD5x4sSb8fHxVgBw9OjRVn369ClavHixIwCkpqYmrV+//kJERIRncXExAcCJEyesN2zYcPHIkSOpn3zyiaOlpaUqNTU1af78+VlJSUmtqh4vJydH+vvvv7cJDQ0tAIDw8PC8rVu32gNASUkJ7d+/3zY8PPzmnDlzXPr161dw5syZswcPHkyZN2+eW0FBgWTJkiXtMjIyzBMTE5NSU1OTJk2alFtf56KpNNg1dyKSAvgSwAAAVwD8Q0TbhRBJOtUuAugrhLhJRKEAVgEIaqg+McZYc9NSUr527dpVXl5eLikuLpbk5+ebyOVyBQB88MEHV4YPH16gW7dfv35FS5cudU5OTjZzc3Mrs7KyEkIIys/PlyQmJrbq27fv7S+++MLxtddeywbUSW3at29ffvr0aQsA6NOnT4GTk1MloB7pR0VFZQNAUFBQiUwmu+uLTEVFBV544YWOERER1xUKRTkA9O3bt7i4uFgSHx9vnpCQYNmtW7fb7dq1qzxw4EDrPXv2tFm2bJkzoE7Sk5aWZrZv377WkZGRN0xNTQEA2mO3ZA15Q10vAGlCiAsAQEQbATwH4E5wF0Ic0ql/BIBbA/aHMcaanZaS8jUhISEZUF9z/+abbxyqrktfpU9lBQUFJps3b24TFBRUBABdu3a9/d///retm5tbma2trUqImm8vsLKyUum+J6r2dgEAwNixYz07duxYOn/+/Gzd7cOGDcuLiYmxT0lJsRw1alQeoD7XmzdvTgsICCjTrSuEABEZdL9DS9GQ0/KuAHSzEl3RbKvJKwB+acD+MMZYs2OsKV+7d+9eFB0d7RgSEnIbAIKDg2+vXLnSMTAwsAgAQkJCitatW2cPAAkJCeZZWVlmXbt2La3ajm69f/75xyI1NfVO1rqoqKj2BQUF0q+//vqeDHjjx4/P27x5s8OhQ4dsxowZcwsA+vXrV7BkyRInlUr93eHvv/+2BICnnnqqYOXKle209x1cv3697rfxNzMNOXKv7qtWtd+MiKgf1ME9pIbyCAARAODh4VFf/WOMsVofXWtoxpryNTg4uCg2NtZWG9yfeOKJosjISPPevXvfBoBZs2Zlh4eHd5DJZAqpVIro6Oh0S0vLe2LEm2++mT169GgvmUym8PX1Lfb3978NAOfPnzf94osvXLy8vEp9fX0VABAREZE9c+bMHADo0aNHqYWFhcrf37+4devWKgBYtGhRZkREhIdcLlcIIcjNza1s//79aa+//vqN1NRUc7lc7mtiYiImTJhwY86cOTeq9qUlIX1TIw/UMFEwgHeFEE9r3v8bAIQQH1Wp1xXATwBChRCptbUbGBgo4uLiDO7H9I3xtdbx6VDtZag7OjuY19rGI1un6S1vI/fS34BTLeUAXi+9pbc8wKn2L92eti56y/u7Pq23nM/n3VrC+aztXALN43zWdi6B2s9nVUR0XAgRqLstPj4+PSAgIKdODTHWTMXHx7cNCAjwrLq9Iafl/wHgTUReRGQGYDSA7boViMgDwFYA4YYEdsYYY4zVrsGm5YUQSiKaBmAPACmANUKIRCKK1JSvBDAfgAOA5ZobJpRVv2UzxhhjrG4adPlZIcRuALurbFup83oSgEkN2QfGGKtCpVKpSCKRGNXd0ezho1KpCICqujJeoY4x9rA5c+PGDVvNf4yMtUgqlYpu3LhhC+BMdeWcOIYx9lBRKpWTrl27tvratWt+4AEOa7lUAM4olcpqZ785uDPGHio9evTIBjC0qfvBWEPib62MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkeHgzhhjjBkZDu6MMcaYkWnQ4E5Eg4gohYjSiGh2NeVERMs05QlE9EhD9ocxxhh7GDRYcCciKYAvAYQCUAAYQ0SKKtVCAXhrfiIArGio/jDGGGMPi4YcufcCkCaEuCCEKAewEcBzVeo8ByBGqB0B0IaIXBqwT4wxxpjRM2nAtl0BXNZ5fwVAkAF1XAFk6VYiogioR/YAUEREKfXb1TprCyCniftgTPh81i8+n//Toak7wFhTaMjgTtVsE/dRB0KIVQBW1Uen6gMRxQkhApu6H8aCz2f94vPJGGvIafkrANx13rsByLyPOowxxhirg4YM7v8A8CYiLyIyAzAawPYqdbYDGK+5a/5RAPlCiKyqDTHGGGPMcA02LS+EUBLRNAB7AEgBrBFCJBJRpKZ8JYDdAJ4BkAagGMDEhupPPWs2lwiMBJ/P+sXnk7GHHAlxzyVuxhhjjLVgvEIdY4wxZmQ4uDPGGGNGxiiDOxFVEtEpnR9PIjpUxzZmEJFVDWUHiMigR42I6Aki2lnHYxvcfn0gorlElKhZAvgUEVVdj6Ax+lDtearr+avrubuf309dEZGDzt/iNSK6qnl9i4iSathnIRE9ZUDbRnveGGP3ryGfc29KJUKIblW29a5aiYikQojKGtqYAWAd1Df6GS0iCgYwGMAjQogyImoLwKyJu2VUhBC5ALoBABG9C6BICPEJEXkCqDZACiHmV7e9lr9ZxhgDYKQj9+oQUZHm3yeIaD8RrQdwmohaEdEuIoonojNENIqIogC0B7CfiPYb2L4nER0kohOaH90vE62J6CciSiKilUQk0ewzkIgOa+r/SETW9f25DeACIEcIUQYAQogcIUSmpn89iCiWiI4T0R7t0sBE1JmI9mrO2Qki6qR5nPFjzTk8TUSjNHWf0IwKNxNRMhF9T0SkKRuk2fYXgBfq0mkimk9E/2iOt0rbpsY4IjqkKeulqd+KiNZo9jlJRFWXQm4qUiL6SjNz8hsRWQIAEa0lohGa1+maz/sXgDA+b4yx2hhrcLfUmQb9qZryXgDmCiEUAAYByBRCBAgh/AD8KoRYBvViOv2EEP0MPGY2gAFCiEcAjAKwrMrx3gDgD6ATgBc0I+R5AJ7S7BMHYGbdP+oD+w2AOxGlEtFyIuoLAERkCuALACOEED0ArAHwgWaf7wF8KYQIgHpGJAvqINMNQACApwB8TP/LE9Ad6pkQBYCOAB4jIgsAXwEYAqAPAOc69vu/Qoiemt+ZJdSzD1qthBC9AUzV9BsA5gLYJ4ToCaCfpn+t6njMhuAN9bn0BXALwPAa6pUKIUIA/Aw+b4yxWjxM0/K6jgkhLmpenwbwCREtBrBTCHHwPo9pCuC/RNQNQCUAWZXjXQAAItoAIARAKdTB7m/N4MkMwOH7PPZ9E0IUEVEPqANFPwA/kDo9bxwAPwC/a/onBZBFRDYAXIUQP2n2LwUAIgoBsEEzZXydiGIB9ARQAPXnv6KpdwqAJ4AiABeFEOc029fhf/kDDNGPiGYBsAJgDyARwA5N2QZN3/4kotZE1AbAQABDiehNTR0LAB51OF5DuSiEOKV5fRzqc1OdHzT/ysHnjTFWC2MN7rW5rX0hhEjVBLdnAHxERL8JIRbeR5uvA7gO9chVAnXwvnOYKnUF1Ovq/y6EGHMfx6pXmoB8AMABIjoNYALUgSZRCBGsW5eIWtfQTHV5ArTKdF5X4n9/d/e1yIJm1L8cQKAQ4jKpr2Nb6FSp6XwPF0LclXSIiJzupw/1qOq5sayh3m2d13zeGGN6Geu0vMGIqD2AYiHEOgCfAHhEU1QIwKYOTdkCyBJCqACEQz3S1epF6mV4JVBP2f8F4AjU09OdNf2wIiJZ1UYbGhF1ISJvnU3dAGQASAHQjtQ33IGITInIVwhRAOAKEQ3TbDcn9VMFfwIYRURSImoH4HEAx/QcOhmAFxF10ryvy5ccbUDKIfV9CiOqlGuv94dAvaRxPtQrJb6mc72/ex2O15zweWOM1ephHbnr8of6OqIKQAWAKZrtqwD8QkRZNVx330VEFZrXhwHMAbCFiMIA7MfdI63DABZpjvUngJ+EECoiegnABiIy19SbByC1/j6aQawBfKGZglVCvRRwhBCiXHND1zIisoX6b+UzqKdxwwFEE9FCqM9ZGICfAAQDiId6xDdLCHGNiOTVHVQIUUrqVL67iCgH6i88fjX08UkiuqLzPgzq686nAaRDncdA101SP/rYGsDLmm3/0fQ/QROo0nH39eYWgc8bY8wQvPwsY4wxZmQe+ml5xhhjzNhwcGeMMcaMDAd3xhhjzMhwcGeMMcaMDAd3xhhjzMhwcGeMMcaMDAd3xhhjzMj8P5hnWMioigv2AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "color = plt.cm.tab20c(np.linspace(0, 0.54, 9))\n",
    "ysr=['First Label', 'Second Label', 'Third Label']\n",
    "x1 = [1,11,21] # x轴点效率位置\n",
    "x2 = [i + 1 for i in x1],
    "x3 = [i + 2 for i in x1],
    "x4 = [i + 3 for i in x1],
    "x5 = [i + 4 for i in x1],
    "x6 = [i + 5 for i in x1],
    "x7 = [i + 6 for i in x1],
    "x8 = [i + 7 for i in x1],
    "x9 = [i + 8 for i in x1],
    "\n",
    "y1 = [i[0] for i in F1],
    "y2 = [i[1] for i in F1],
    "y3 = [i[2] for i in F1],
    "y4 = [i[3] for i in F1],
    "y5 = [i[4] for i in F1],
    "y6 = [i[5] for i in F1],
    "y7 = [i[6] for i in F1],
    "y8 = [i[7] for i in F1],
    "y9 = [i[8] for i in F1],
    "ax1 = plt.subplot(1,1,1)\n",
    "plt.sca(ax1)\n",
    "plt.xlabel(\"\") #X轴标签\n",
    "plt.ylabel(\"T est F1-Score\")  #Y轴标签\n",
    "plt.bar(x1, y1, alpha=0.7, width=1, label=\"Bag Vector + Naive Bayes\", color = color[0])\n",
    "plt.bar(x2, y2, alpha=0.7, width=1, label=\"Bag Vector + Random Forest\", color = color[1])\n",
    "plt.bar(x3, y3, alpha=0.7, width=1, label=\"Bag Vector + SVM\", color = color[2])\n",
    "plt.bar(x4, y4, alpha=0.7, width=1, label=\"Tf-idf + Naive Bayes\", color = color[3])\n",
    "plt.bar(x5, y5, alpha=0.7, width=1, label=\"Tf-idf + Random Forest\", color = color[4])\n",
    "plt.bar(x6, y6, alpha=0.7, width=1, label=\"Tf-idf + SVM\", color = color[5])\n",
    "plt.bar(x7, y7, alpha=0.7, width=1, label=\"FastText\", color = color[6])\n",
    "plt.bar(x8, y8, alpha=0.7, width=1, label=\"TextCNN\", color = color[7])\n",
    "plt.bar(x9, y9, alpha=0.7, width=1, label=\"TextCNN + Word2Vec\", color = color[8])\n",
    "ax1.set_xticks(x3)\n",
    "ax1.set_xticklabels(ysr)\n",
    "box = ax1.get_position()\n",
    "ax1.set_position([box.x0, box.y0, box.width* 0.8 , box.height])\n",
    "ax1.legend(loc='center left', bbox_to_anchor=(1, 0.5),ncol=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918a5d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
