{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a806496d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import codecs\n",
    "import jieba\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,classification_report,f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "random.seed(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd879a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/_t/zp2z4_4x4gg78j4gh86h6z5m0000gn/T/jieba.cache\n",
      "Loading model cost 0.847 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>商品名称</th>\n",
       "      <th>一级分类</th>\n",
       "      <th>二级分类</th>\n",
       "      <th>三级分类</th>\n",
       "      <th>一级分类_整数</th>\n",
       "      <th>二级分类_整数</th>\n",
       "      <th>三级分类_整数</th>\n",
       "      <th>segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>**N蓝妹啤酒易拉罐3</td>\n",
       "      <td>酒类</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>啤酒</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[N, 蓝妹, 啤酒, 易拉罐]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>农夫水溶C100青皮桔445ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>果蔬汁</td>\n",
       "      <td>果蔬饮料</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[农夫, 水溶, C100, 青皮, 桔, 445ml]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N七匹狼（软灰）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[N, 七匹狼, 软灰]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N七匹狼（软红）</td>\n",
       "      <td>烟类</td>\n",
       "      <td>香烟</td>\n",
       "      <td>软盒香烟</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>[N, 七匹狼, 软红]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>农夫山泉水550ml</td>\n",
       "      <td>饮料</td>\n",
       "      <td>水</td>\n",
       "      <td>矿泉水</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>[农夫山泉, 水, 550ml]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               商品名称 一级分类 二级分类  三级分类  一级分类_整数  二级分类_整数  三级分类_整数  \\\n",
       "0       **N蓝妹啤酒易拉罐3   酒类   啤酒    啤酒        0        0        0   \n",
       "1  农夫水溶C100青皮桔445ml   饮料  果蔬汁  果蔬饮料        1        1        1   \n",
       "2          N七匹狼（软灰）   烟类   香烟  软盒香烟        2        2        2   \n",
       "3          N七匹狼（软红）   烟类   香烟  软盒香烟        2        2        2   \n",
       "4        农夫山泉水550ml   饮料    水   矿泉水        1        3        3   \n",
       "\n",
       "                        segment  \n",
       "0              [N, 蓝妹, 啤酒, 易拉罐]  \n",
       "1  [农夫, 水溶, C100, 青皮, 桔, 445ml]  \n",
       "2                  [N, 七匹狼, 软灰]  \n",
       "3                  [N, 七匹狼, 软红]  \n",
       "4              [农夫山泉, 水, 550ml]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#读文件\n",
    "data = pd.read_csv('/Users/mac/Desktop/2022春课件/时序/商品清单（new）.csv')\n",
    "#标签映射为整数\n",
    "listType = data['一级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['一级分类_整数'] = data['一级分类'].map(com_map)\n",
    "\n",
    "listType = data['二级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['二级分类_整数'] = data['二级分类'].map(com_map)\n",
    "\n",
    "listType = data['三级分类'].unique()\n",
    "com_map = dict.fromkeys(listType)\n",
    "for i in range(len(listType)):\n",
    "    com_map[listType[i]] = i\n",
    "data['三级分类_整数'] = data['三级分类'].map(com_map)\n",
    "#去停用词\n",
    "stopkey = [w.strip() for w in codecs.open('/Users/mac/Downloads/呆萌的停用词表.txt', 'r').readlines()]\n",
    "data['segment'] = data['商品名称'].apply(lambda x:jieba.lcut(x))\n",
    "for i in range(len(data)):\n",
    "    words = data['segment'][i].copy()\n",
    "    for x in words:\n",
    "        if x in stopkey:\n",
    "            data['segment'][i].remove(x)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fd4a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=123)\n",
    "# for train_index, test_index in split.split(data, data['三级分类_整数']):\n",
    "#     train_set = data[data.index.isin(train_index)]\n",
    "#     test_set = data[data.index.isin(test_index)]\n",
    "\n",
    "\n",
    "from tensorflow import keras\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(data['segment'])\n",
    "vocab=tokenizer.word_index\n",
    "X = data['segment']\n",
    "maxlen = 30\n",
    "Y = data['三级分类_整数']\n",
    "kinds = len(Y.unique())\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357e7e4c",
   "metadata": {},
   "source": [
    "#### 1、focal_loss损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41ffc4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "def focal_loss(y_true, y_pred):\n",
    "    gamma = 2.0\n",
    "    alpha = 0.25\n",
    "    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n",
    "    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n",
    "    return -keras.backend.sum(alpha * keras.backend.pow(1. - pt_1, gamma) * keras.backend.log(pt_1))-keras.backend.sum((1-alpha) * keras.backend.pow( pt_0, gamma) * keras.backend.log(1. - pt_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e288ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 30, 300)      3864600     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 29, 128)      76928       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 28, 128)      115328      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 27, 64)       76864       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 1, 128)       0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 1, 128)       0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 1, 64)        0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 1, 320)       0           max_pooling1d[0][0]              \n",
      "                                                                 max_pooling1d_1[0][0]            \n",
      "                                                                 max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 320)          0           concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 320)          0           flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 133)          42693       dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 4,176,413\n",
      "Trainable params: 4,176,413\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(128, 2, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(128, 3, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(64, 4, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_loss = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_loss.compile(loss=[focal_loss], optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_loss.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc03dd85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8383 samples, validate on 2096 samples\n",
      "Epoch 1/10\n",
      "8383/8383 [==============================] - 16s 2ms/sample - loss: 214.6252 - accuracy: 0.0885 - val_loss: 190.9690 - val_accuracy: 0.1889\n",
      "Epoch 2/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 171.7551 - accuracy: 0.2795 - val_loss: 145.8718 - val_accuracy: 0.3989\n",
      "Epoch 3/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 111.9150 - accuracy: 0.5624 - val_loss: 99.3431 - val_accuracy: 0.5935\n",
      "Epoch 4/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 64.4253 - accuracy: 0.7522 - val_loss: 74.3327 - val_accuracy: 0.6870\n",
      "Epoch 5/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 36.8153 - accuracy: 0.8547 - val_loss: 61.6883 - val_accuracy: 0.7371\n",
      "Epoch 6/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 21.4092 - accuracy: 0.9141 - val_loss: 54.4145 - val_accuracy: 0.7557\n",
      "Epoch 7/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 11.9081 - accuracy: 0.9483 - val_loss: 51.3185 - val_accuracy: 0.7700\n",
      "Epoch 8/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 7.4076 - accuracy: 0.9676 - val_loss: 48.8824 - val_accuracy: 0.7777\n",
      "Epoch 9/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 4.9005 - accuracy: 0.9758 - val_loss: 48.4450 - val_accuracy: 0.7848\n",
      "Epoch 10/10\n",
      "8383/8383 [==============================] - 14s 2ms/sample - loss: 3.7579 - accuracy: 0.9797 - val_loss: 47.8969 - val_accuracy: 0.7863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x153c5d950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_loss.fit(X_train_padded_seqs, one_hot_labels,  epochs=10, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "938c7a19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 0.776046304541407\n",
      "平均f1-score: 0.7777820684188397\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        52\n",
      "           1       0.80      0.74      0.77        80\n",
      "           2       0.79      0.79      0.79        24\n",
      "           3       0.78      1.00      0.88        18\n",
      "           4       0.93      0.94      0.94       158\n",
      "           5       0.86      0.85      0.85        85\n",
      "           6       0.81      0.77      0.79       140\n",
      "           7       0.84      0.67      0.74        39\n",
      "           8       0.29      0.64      0.40        11\n",
      "           9       0.93      0.83      0.88       119\n",
      "          10       0.88      0.90      0.89        31\n",
      "          11       0.81      0.72      0.76       141\n",
      "          12       0.74      0.86      0.79        29\n",
      "          13       0.84      0.86      0.85        50\n",
      "          14       0.75      0.75      0.75         4\n",
      "          15       0.54      0.73      0.62        26\n",
      "          16       0.89      0.91      0.90       172\n",
      "          17       0.78      0.91      0.84        96\n",
      "          18       1.00      1.00      1.00        12\n",
      "          19       0.83      0.92      0.87        26\n",
      "          20       0.81      0.94      0.87        68\n",
      "          21       0.72      0.79      0.75        72\n",
      "          22       0.67      0.56      0.61       112\n",
      "          23       0.68      0.63      0.65       158\n",
      "          24       0.80      0.92      0.86        49\n",
      "          25       0.82      0.80      0.81       140\n",
      "          26       0.59      0.44      0.50        39\n",
      "          27       0.77      0.83      0.80        12\n",
      "          28       0.88      0.90      0.89       121\n",
      "          29       0.64      0.74      0.69        61\n",
      "          30       0.30      0.75      0.43         4\n",
      "          31       0.95      0.95      0.95        58\n",
      "          32       0.75      1.00      0.86         3\n",
      "          33       0.81      0.88      0.84        24\n",
      "          34       0.93      0.87      0.90        30\n",
      "          35       0.50      0.75      0.60         4\n",
      "          36       0.77      0.65      0.70        31\n",
      "          37       0.80      0.85      0.83       103\n",
      "          38       0.67      1.00      0.80         6\n",
      "          39       0.89      0.86      0.88       130\n",
      "          40       0.93      0.97      0.95        39\n",
      "          41       0.75      1.00      0.86         3\n",
      "          42       0.50      1.00      0.67         6\n",
      "          43       0.79      1.00      0.88        11\n",
      "          44       1.00      0.89      0.94        28\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       0.59      0.79      0.68        29\n",
      "          47       0.67      0.80      0.73        10\n",
      "          48       0.92      0.79      0.85        14\n",
      "          49       0.78      1.00      0.88        14\n",
      "          50       0.56      0.50      0.53        44\n",
      "          51       0.56      1.00      0.72        14\n",
      "          52       0.30      0.23      0.26        13\n",
      "          53       1.00      0.88      0.93         8\n",
      "          54       0.68      0.67      0.68        78\n",
      "          55       0.67      1.00      0.80         2\n",
      "          56       0.25      0.14      0.18         7\n",
      "          57       1.00      0.57      0.73         7\n",
      "          58       0.88      1.00      0.93        14\n",
      "          59       0.71      1.00      0.83         5\n",
      "          60       0.40      0.50      0.44         4\n",
      "          61       0.77      0.65      0.71        26\n",
      "          62       0.80      0.72      0.76        39\n",
      "          63       0.68      0.78      0.72        32\n",
      "          64       0.76      0.72      0.74        65\n",
      "          65       0.70      0.73      0.71        22\n",
      "          66       0.87      0.85      0.86       111\n",
      "          67       1.00      0.84      0.91        19\n",
      "          68       0.43      0.30      0.35        10\n",
      "          69       0.50      0.33      0.40         6\n",
      "          70       0.57      0.52      0.55        23\n",
      "          71       0.65      0.73      0.69        77\n",
      "          72       0.79      0.92      0.85        12\n",
      "          73       0.86      0.67      0.75         9\n",
      "          74       0.70      0.75      0.72        71\n",
      "          75       0.79      0.79      0.79        28\n",
      "          76       0.54      0.56      0.55        27\n",
      "          77       0.84      0.96      0.90        27\n",
      "          78       0.75      0.75      0.75         8\n",
      "          79       0.93      1.00      0.96        27\n",
      "          80       0.88      1.00      0.94        23\n",
      "          81       0.85      0.52      0.65        21\n",
      "          82       0.79      0.85      0.81        13\n",
      "          83       0.67      1.00      0.80         4\n",
      "          84       0.50      1.00      0.67         4\n",
      "          85       0.62      0.65      0.63        82\n",
      "          86       1.00      1.00      1.00         1\n",
      "          87       0.72      0.82      0.77        57\n",
      "          88       0.82      0.82      0.82        11\n",
      "          89       0.86      0.83      0.85        36\n",
      "          90       0.81      0.95      0.88        37\n",
      "          91       1.00      0.91      0.95        11\n",
      "          92       0.44      0.58      0.50        12\n",
      "          93       0.83      0.60      0.70        25\n",
      "          94       0.56      0.67      0.61        15\n",
      "          95       0.77      0.93      0.84        29\n",
      "          96       0.36      0.80      0.50         5\n",
      "          97       0.64      0.88      0.74         8\n",
      "          98       0.84      0.93      0.89        46\n",
      "          99       0.88      0.49      0.63       185\n",
      "         100       0.82      1.00      0.90         9\n",
      "         101       0.91      0.94      0.92        31\n",
      "         102       0.67      1.00      0.80         4\n",
      "         103       0.75      0.53      0.62       121\n",
      "         104       0.12      0.20      0.15         5\n",
      "         106       0.44      1.00      0.62         4\n",
      "         107       0.82      0.94      0.88        35\n",
      "         108       0.53      0.73      0.62        11\n",
      "         109       0.43      0.75      0.55         4\n",
      "         110       0.67      0.74      0.70        19\n",
      "         111       0.00      0.00      0.00         0\n",
      "         112       0.80      0.87      0.83        23\n",
      "         113       0.00      0.00      0.00         0\n",
      "         114       0.00      0.00      0.00         1\n",
      "         115       0.60      0.67      0.63         9\n",
      "         116       0.00      0.00      0.00         0\n",
      "         117       0.00      0.00      0.00         2\n",
      "         118       1.00      1.00      1.00         2\n",
      "         119       0.64      0.64      0.64        11\n",
      "         120       0.00      0.00      0.00         0\n",
      "         121       0.68      0.87      0.76        15\n",
      "         122       0.29      0.20      0.24        10\n",
      "         123       0.00      0.00      0.00         7\n",
      "         124       0.83      1.00      0.91         5\n",
      "         125       0.25      0.20      0.22         5\n",
      "         126       0.00      0.00      0.00         0\n",
      "         128       0.00      0.00      0.00         0\n",
      "         129       0.00      0.00      0.00         0\n",
      "         130       0.00      0.00      0.00         0\n",
      "         131       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78      4492\n",
      "   macro avg       0.66      0.71      0.68      4492\n",
      "weighted avg       0.79      0.78      0.77      4492\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_loss.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b425338",
   "metadata": {},
   "source": [
    "#### 2、BorderlineSMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d3545b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10567, 30), (10567, 133), (12586, 30), (12586, 133))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#重复过少的样本，使其不少于6个，以便进行后续SMOTE\n",
    "while(min(data['三级分类_整数'].value_counts())<10):\n",
    "    data = data.append(data[data['三级分类_整数'].isin(data['三级分类_整数'].value_counts()[data['三级分类_整数'].value_counts()<10].index)])\n",
    "X = data['segment']\n",
    "Y = data['三级分类_整数']\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.7, random_state = 123)\n",
    "X_train_word_ids=tokenizer.texts_to_sequences(X_train)\n",
    "X_test_word_ids = tokenizer.texts_to_sequences(X_test)\n",
    "#将超过固定值的部分截掉，不足的在最前面用0填充\n",
    "X_train_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_train_word_ids, maxlen = maxlen)\n",
    "X_test_padded_seqs=keras.preprocessing.sequence.pad_sequences(X_test_word_ids,  maxlen = maxlen)\n",
    "#将标签转换为one-hot编码\n",
    "one_hot_labels = keras.utils.to_categorical(Y_train, num_classes=kinds)\n",
    "\n",
    "Y_dict = dict(Y_train.value_counts())\n",
    "for key in Y_dict:\n",
    "    if(Y_dict[key]<50):\n",
    "        Y_dict[key] = 50\n",
    "X_resample, Y_resample = SMOTE(random_state = 123, k_neighbors=2, sampling_strategy = Y_dict).fit_resample(X_train_padded_seqs, one_hot_labels)\n",
    "X_train_padded_seqs.shape, one_hot_labels.shape, X_resample.shape, Y_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a08e09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 30, 300)      3864600     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 29, 128)      76928       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 28, 128)      115328      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_5 (Conv1D)               (None, 27, 128)      153728      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1D)  (None, 1, 128)       0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1D)  (None, 1, 128)       0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_5 (MaxPooling1D)  (None, 1, 128)       0           conv1d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 1, 384)       0           max_pooling1d_3[0][0]            \n",
      "                                                                 max_pooling1d_4[0][0]            \n",
      "                                                                 max_pooling1d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 384)          0           flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 133)          51205       dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 4,261,789\n",
      "Trainable params: 4,261,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(128, 2, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(128, 3, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(128, 4, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_balence = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_balence.compile(loss=[focal_loss], optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_balence.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc9ab195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10068 samples, validate on 2518 samples\n",
      "Epoch 1/10\n",
      "10068/10068 [==============================] - 20s 2ms/sample - loss: 210.8719 - accuracy: 0.0938 - val_loss: 279.8426 - val_accuracy: 0.0310\n",
      "Epoch 2/10\n",
      "10068/10068 [==============================] - 18s 2ms/sample - loss: 152.4773 - accuracy: 0.3605 - val_loss: 265.2306 - val_accuracy: 0.1009\n",
      "Epoch 3/10\n",
      "10068/10068 [==============================] - 18s 2ms/sample - loss: 85.2078 - accuracy: 0.6605 - val_loss: 245.9271 - val_accuracy: 0.1628\n",
      "Epoch 4/10\n",
      "10068/10068 [==============================] - 18s 2ms/sample - loss: 44.0970 - accuracy: 0.8172 - val_loss: 211.2485 - val_accuracy: 0.2812\n",
      "Epoch 5/10\n",
      "10068/10068 [==============================] - 18s 2ms/sample - loss: 22.7623 - accuracy: 0.9038 - val_loss: 182.4055 - val_accuracy: 0.3384\n",
      "Epoch 6/10\n",
      "10068/10068 [==============================] - 19s 2ms/sample - loss: 12.2157 - accuracy: 0.9444 - val_loss: 167.5354 - val_accuracy: 0.3944\n",
      "Epoch 7/10\n",
      "10068/10068 [==============================] - 19s 2ms/sample - loss: 6.9489 - accuracy: 0.9671 - val_loss: 165.3958 - val_accuracy: 0.4047\n",
      "Epoch 8/10\n",
      "10068/10068 [==============================] - 19s 2ms/sample - loss: 4.5412 - accuracy: 0.9748 - val_loss: 166.1737 - val_accuracy: 0.4102\n",
      "Epoch 9/10\n",
      "10068/10068 [==============================] - 19s 2ms/sample - loss: 3.3571 - accuracy: 0.9818 - val_loss: 170.3642 - val_accuracy: 0.4083\n",
      "Epoch 10/10\n",
      "10068/10068 [==============================] - 19s 2ms/sample - loss: 2.7819 - accuracy: 0.9829 - val_loss: 169.5883 - val_accuracy: 0.4095\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x15a77b310>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_balence.fit(X_resample, Y_resample,  epochs=10, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ece1ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试准确率 0.8\n",
      "平均f1-score: 0.7991523026600292\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97        56\n",
      "           1       0.80      0.80      0.80        79\n",
      "           2       0.77      0.95      0.85        21\n",
      "           3       0.82      1.00      0.90        28\n",
      "           4       0.97      0.90      0.94       173\n",
      "           5       0.89      0.90      0.89        89\n",
      "           6       0.82      0.87      0.84       121\n",
      "           7       0.81      0.69      0.75        32\n",
      "           8       0.32      0.88      0.47         8\n",
      "           9       0.90      0.85      0.88       107\n",
      "          10       0.91      0.94      0.92        32\n",
      "          11       0.81      0.70      0.75       149\n",
      "          12       0.91      0.85      0.88        34\n",
      "          13       0.87      0.85      0.86        61\n",
      "          14       0.80      0.80      0.80         5\n",
      "          15       0.76      0.70      0.73        37\n",
      "          16       0.96      0.91      0.94       199\n",
      "          17       0.87      0.85      0.86       109\n",
      "          18       1.00      1.00      1.00        14\n",
      "          19       0.86      0.89      0.88        28\n",
      "          20       0.78      0.85      0.82        60\n",
      "          21       0.81      0.83      0.82        66\n",
      "          22       0.63      0.69      0.66       103\n",
      "          23       0.76      0.60      0.67       171\n",
      "          24       0.83      0.90      0.87        50\n",
      "          25       0.86      0.88      0.87       144\n",
      "          26       0.48      0.75      0.59        20\n",
      "          27       0.82      0.90      0.86        10\n",
      "          28       0.91      0.87      0.89       130\n",
      "          29       0.76      0.65      0.70        80\n",
      "          30       0.62      0.71      0.67         7\n",
      "          31       0.93      0.92      0.92        60\n",
      "          32       0.60      1.00      0.75         3\n",
      "          33       0.84      0.95      0.89        22\n",
      "          34       0.95      0.86      0.90        22\n",
      "          35       0.67      0.50      0.57         4\n",
      "          36       0.87      0.81      0.84        32\n",
      "          37       0.81      0.85      0.83       108\n",
      "          38       0.56      1.00      0.71         5\n",
      "          39       0.93      0.95      0.94       130\n",
      "          40       0.91      0.97      0.94        32\n",
      "          41       0.33      1.00      0.50         1\n",
      "          42       0.60      1.00      0.75         3\n",
      "          43       0.75      0.75      0.75        12\n",
      "          44       1.00      0.86      0.93        22\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       0.85      0.81      0.83        42\n",
      "          47       0.67      0.80      0.73         5\n",
      "          48       0.91      0.83      0.87        12\n",
      "          49       0.85      0.94      0.89        18\n",
      "          50       0.39      0.75      0.52        20\n",
      "          51       0.81      0.91      0.86        23\n",
      "          52       0.26      0.22      0.24        23\n",
      "          53       0.90      1.00      0.95         9\n",
      "          54       0.67      0.68      0.68        78\n",
      "          55       0.50      0.75      0.60         4\n",
      "          56       0.50      0.50      0.50         8\n",
      "          57       0.33      0.25      0.29         4\n",
      "          58       0.70      0.93      0.80        15\n",
      "          59       0.86      0.60      0.71        10\n",
      "          60       0.86      0.60      0.71        10\n",
      "          61       0.74      0.74      0.74        23\n",
      "          62       0.78      0.67      0.72        42\n",
      "          63       0.76      0.81      0.78        36\n",
      "          64       0.89      0.75      0.81        64\n",
      "          65       0.70      0.67      0.68        24\n",
      "          66       0.87      0.83      0.85       103\n",
      "          67       0.91      0.77      0.83        13\n",
      "          68       0.57      0.31      0.40        13\n",
      "          69       0.50      0.25      0.33         4\n",
      "          70       0.53      0.55      0.54        29\n",
      "          71       0.73      0.63      0.67        89\n",
      "          72       0.79      0.94      0.86        16\n",
      "          73       0.75      0.75      0.75         8\n",
      "          74       0.71      0.85      0.78        67\n",
      "          75       0.88      0.75      0.81        28\n",
      "          76       0.62      0.52      0.56        31\n",
      "          77       0.90      0.90      0.90        31\n",
      "          78       0.69      1.00      0.82         9\n",
      "          79       1.00      0.96      0.98        23\n",
      "          80       0.85      1.00      0.92        23\n",
      "          81       0.31      0.80      0.44         5\n",
      "          82       0.75      0.75      0.75        12\n",
      "          83       0.33      1.00      0.50         1\n",
      "          84       0.80      1.00      0.89         4\n",
      "          85       0.71      0.67      0.69        90\n",
      "          86       0.50      1.00      0.67         2\n",
      "          87       0.76      0.85      0.81        61\n",
      "          88       0.75      0.67      0.71         9\n",
      "          89       0.94      0.81      0.87        42\n",
      "          90       0.90      0.90      0.90        42\n",
      "          91       0.83      1.00      0.91         5\n",
      "          92       0.33      0.43      0.38        14\n",
      "          93       0.67      0.71      0.69        17\n",
      "          94       0.65      0.93      0.76        14\n",
      "          95       0.67      0.91      0.77        22\n",
      "          96       0.43      1.00      0.60         6\n",
      "          97       1.00      0.83      0.91         6\n",
      "          98       0.86      0.93      0.89        40\n",
      "          99       0.83      0.51      0.63       154\n",
      "         100       0.78      0.88      0.82         8\n",
      "         101       0.88      1.00      0.93        28\n",
      "         102       0.69      0.75      0.72        12\n",
      "         103       0.70      0.63      0.66        95\n",
      "         104       0.25      0.40      0.31         5\n",
      "         105       1.00      1.00      1.00         7\n",
      "         106       0.50      0.83      0.62         6\n",
      "         107       0.89      0.94      0.91        33\n",
      "         108       0.44      0.88      0.58         8\n",
      "         109       0.57      1.00      0.73         4\n",
      "         110       0.62      0.81      0.70        16\n",
      "         111       1.00      1.00      1.00         6\n",
      "         112       0.64      0.95      0.76        22\n",
      "         113       1.00      1.00      1.00         5\n",
      "         114       1.00      0.25      0.40         4\n",
      "         115       0.60      0.75      0.67         8\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.20      1.00      0.33         1\n",
      "         118       0.50      1.00      0.67         1\n",
      "         119       0.58      1.00      0.74         7\n",
      "         120       1.00      1.00      1.00         6\n",
      "         121       0.81      0.85      0.83        26\n",
      "         122       0.11      0.17      0.13         6\n",
      "         123       0.00      0.00      0.00         0\n",
      "         124       0.38      1.00      0.55         3\n",
      "         125       0.33      0.25      0.29         4\n",
      "         126       0.00      0.00      0.00         0\n",
      "         127       1.00      1.00      1.00         3\n",
      "         128       1.00      1.00      1.00         5\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       1.00      1.00      1.00         1\n",
      "         131       1.00      1.00      1.00         4\n",
      "         132       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.80      4530\n",
      "   macro avg       0.73      0.80      0.75      4530\n",
      "weighted avg       0.81      0.80      0.80      4530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_balence.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('测试准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3867f6b4",
   "metadata": {},
   "source": [
    "##### 寻找最优的上采样数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afac7a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in [60, 65, 70]:\n",
    "    Y_dict = dict(Y_train.value_counts())\n",
    "    for key in Y_dict:\n",
    "        if(Y_dict[key]<i):\n",
    "            Y_dict[key] = i\n",
    "    X_resample, Y_resample = SMOTE(random_state = 123, k_neighbors=2, sampling_strategy = Y_dict).fit_resample(X_train_padded_seqs, one_hot_labels)\n",
    "    main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "    # 词嵌入（使用预训练的词向量）\n",
    "    embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "    embed = embedder(main_input)\n",
    "    # 词窗大小分别为3,4,5\n",
    "    cnn1 = keras.layers.Conv1D(128, 2, padding='valid', strides=1, activation='relu')(embed)\n",
    "    cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "    cnn2 = keras.layers.Conv1D(128, 3, padding='valid', strides=1, activation='relu')(embed)\n",
    "    cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "    cnn3 = keras.layers.Conv1D(128, 4, padding='valid', strides=1, activation='relu')(embed)\n",
    "    cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "    # 合并三个模型的输出向量\n",
    "    cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "    flat = keras.layers.Flatten()(cnn)\n",
    "    drop = keras.layers.Dropout(0.3)(flat)\n",
    "    main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "    modelCNN_balence2 = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "    modelCNN_balence2.compile(loss=[focal_loss], optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "    modelCNN_balence2.fit(X_resample, Y_resample,  epochs=10, batch_size=200, validation_split = 0.2)\n",
    "    result = modelCNN_balence2.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "    Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "    print('i = ',i)\n",
    "    print('测试准确率', accuracy_score(Y_test, Y_predict))\n",
    "    print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "#60最好"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f56f672",
   "metadata": {},
   "source": [
    "###### 60最好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74312dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10567, 30), (10567, 133), (13330, 30), (13330, 133))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_dict = dict(Y_train.value_counts())\n",
    "for key in Y_dict:\n",
    "    if(Y_dict[key]<60):\n",
    "        Y_dict[key] = 60\n",
    "X_resample, Y_resample = SMOTE(random_state = 123, k_neighbors=2, sampling_strategy = Y_dict).fit_resample(X_train_padded_seqs, one_hot_labels)\n",
    "X_train_padded_seqs.shape, one_hot_labels.shape, X_resample.shape, Y_resample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1a4df47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_20\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_21 (InputLayer)           [(None, 30)]         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_20 (Embedding)        (None, 30, 300)      3864600     input_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_60 (Conv1D)              (None, 29, 128)      76928       embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_61 (Conv1D)              (None, 28, 128)      115328      embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_62 (Conv1D)              (None, 27, 128)      153728      embedding_20[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_60 (MaxPooling1D) (None, 1, 128)       0           conv1d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_61 (MaxPooling1D) (None, 1, 128)       0           conv1d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_62 (MaxPooling1D) (None, 1, 128)       0           conv1d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_20 (Concatenate)    (None, 1, 384)       0           max_pooling1d_60[0][0]           \n",
      "                                                                 max_pooling1d_61[0][0]           \n",
      "                                                                 max_pooling1d_62[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "flatten_20 (Flatten)            (None, 384)          0           concatenate_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "dropout_20 (Dropout)            (None, 384)          0           flatten_20[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_20 (Dense)                (None, 133)          51205       dropout_20[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 4,261,789\n",
      "Trainable params: 4,261,789\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "main_input = keras.layers.Input(shape=(maxlen,), dtype='float64')\n",
    "# 词嵌入（使用预训练的词向量）\n",
    "embedder = keras.layers.Embedding(len(vocab) + 1, 300, input_length=maxlen)\n",
    "embed = embedder(main_input)\n",
    "# 词窗大小分别为3,4,5\n",
    "cnn1 = keras.layers.Conv1D(128, 2, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn1 = keras.layers.MaxPooling1D(pool_size=29)(cnn1)\n",
    "cnn2 = keras.layers.Conv1D(128, 3, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn2 = keras.layers.MaxPooling1D(pool_size=28)(cnn2)\n",
    "cnn3 = keras.layers.Conv1D(128, 4, padding='valid', strides=1, activation='relu')(embed)\n",
    "cnn3 = keras.layers.MaxPooling1D(pool_size=27)(cnn3)\n",
    "# 合并三个模型的输出向量\n",
    "cnn = keras.layers.concatenate([cnn1, cnn2, cnn3], axis=-1)\n",
    "flat = keras.layers.Flatten()(cnn)\n",
    "drop = keras.layers.Dropout(0.3)(flat)\n",
    "main_output = keras.layers.Dense(kinds, activation='softmax')(drop)\n",
    "modelCNN_balence = keras.models.Model(inputs=main_input, outputs=main_output)\n",
    "modelCNN_balence.compile(loss=[focal_loss], optimizer='adam', metrics=['accuracy'], lr=0.0001)\n",
    "modelCNN_balence.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a72bf128",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 10664 samples, validate on 2666 samples\n",
      "Epoch 1/10\n",
      "10664/10664 [==============================] - 23s 2ms/sample - loss: 209.6219 - accuracy: 0.1005 - val_loss: 302.9841 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "10664/10664 [==============================] - 19s 2ms/sample - loss: 147.7778 - accuracy: 0.3766 - val_loss: 295.6922 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "10664/10664 [==============================] - 22s 2ms/sample - loss: 78.9203 - accuracy: 0.6787 - val_loss: 282.0902 - val_accuracy: 0.0518\n",
      "Epoch 4/10\n",
      "10664/10664 [==============================] - 20s 2ms/sample - loss: 40.0333 - accuracy: 0.8321 - val_loss: 243.1570 - val_accuracy: 0.1819\n",
      "Epoch 5/10\n",
      "10664/10664 [==============================] - 23s 2ms/sample - loss: 20.2402 - accuracy: 0.9105 - val_loss: 214.5065 - val_accuracy: 0.2397\n",
      "Epoch 6/10\n",
      "10664/10664 [==============================] - 22s 2ms/sample - loss: 10.9044 - accuracy: 0.9511 - val_loss: 201.2883 - val_accuracy: 0.2926\n",
      "Epoch 7/10\n",
      "10664/10664 [==============================] - 22s 2ms/sample - loss: 6.3426 - accuracy: 0.9672 - val_loss: 201.2113 - val_accuracy: 0.3005\n",
      "Epoch 8/10\n",
      "10664/10664 [==============================] - 21s 2ms/sample - loss: 4.4917 - accuracy: 0.9755 - val_loss: 201.9977 - val_accuracy: 0.3031\n",
      "Epoch 9/10\n",
      "10664/10664 [==============================] - 23s 2ms/sample - loss: 3.3129 - accuracy: 0.9808 - val_loss: 206.4993 - val_accuracy: 0.3046\n",
      "Epoch 10/10\n",
      "10664/10664 [==============================] - 22s 2ms/sample - loss: 2.5799 - accuracy: 0.9838 - val_loss: 209.2386 - val_accuracy: 0.2986\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1d3fcfed0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelCNN_balence.fit(X_resample, Y_resample,  epochs=10, batch_size=200, validation_split = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73df8497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试准确率 0.8026490066225166\n",
      "平均f1-score: 0.8016795808863912\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98        57\n",
      "           1       0.85      0.80      0.82        84\n",
      "           2       0.81      0.95      0.88        22\n",
      "           3       0.88      1.00      0.94        30\n",
      "           4       0.97      0.92      0.95       170\n",
      "           5       0.91      0.92      0.92        89\n",
      "           6       0.85      0.80      0.82       137\n",
      "           7       0.78      0.70      0.74        30\n",
      "           8       0.36      0.80      0.50        10\n",
      "           9       0.93      0.87      0.90       108\n",
      "          10       0.91      0.94      0.92        32\n",
      "          11       0.77      0.78      0.77       127\n",
      "          12       0.81      0.79      0.80        33\n",
      "          13       0.88      0.87      0.88        61\n",
      "          14       0.80      0.29      0.42        14\n",
      "          15       0.74      0.81      0.77        31\n",
      "          16       0.96      0.94      0.95       192\n",
      "          17       0.87      0.89      0.88       105\n",
      "          18       1.00      1.00      1.00        14\n",
      "          19       0.86      0.86      0.86        29\n",
      "          20       0.80      0.84      0.82        62\n",
      "          21       0.81      0.73      0.77        75\n",
      "          22       0.64      0.64      0.64       112\n",
      "          23       0.75      0.66      0.70       154\n",
      "          24       0.83      0.90      0.87        50\n",
      "          25       0.86      0.89      0.87       142\n",
      "          26       0.58      0.75      0.65        24\n",
      "          27       0.82      0.60      0.69        15\n",
      "          28       0.94      0.78      0.85       150\n",
      "          29       0.63      0.77      0.69        56\n",
      "          30       0.62      0.83      0.71         6\n",
      "          31       0.92      0.95      0.93        57\n",
      "          32       0.60      1.00      0.75         3\n",
      "          33       0.76      0.95      0.84        20\n",
      "          34       0.90      0.78      0.84        23\n",
      "          35       0.67      0.67      0.67         3\n",
      "          36       0.93      0.72      0.81        39\n",
      "          37       0.78      0.89      0.83        99\n",
      "          38       0.56      1.00      0.71         5\n",
      "          39       0.91      0.95      0.93       129\n",
      "          40       0.88      0.97      0.92        31\n",
      "          41       0.33      1.00      0.50         1\n",
      "          42       0.60      1.00      0.75         3\n",
      "          43       0.67      0.89      0.76         9\n",
      "          44       1.00      0.86      0.93        22\n",
      "          45       1.00      1.00      1.00         6\n",
      "          46       0.75      0.81      0.78        37\n",
      "          47       0.67      0.80      0.73         5\n",
      "          48       0.91      1.00      0.95        10\n",
      "          49       0.90      0.90      0.90        20\n",
      "          50       0.42      0.64      0.51        25\n",
      "          51       0.88      0.96      0.92        24\n",
      "          52       0.32      0.40      0.35        15\n",
      "          53       0.90      1.00      0.95         9\n",
      "          54       0.68      0.69      0.69        78\n",
      "          55       0.50      0.60      0.55         5\n",
      "          56       0.50      0.40      0.44        10\n",
      "          57       0.33      0.33      0.33         3\n",
      "          58       0.80      0.94      0.86        17\n",
      "          59       1.00      0.50      0.67        14\n",
      "          60       0.71      0.62      0.67         8\n",
      "          61       0.70      0.70      0.70        23\n",
      "          62       0.75      0.87      0.81        31\n",
      "          63       0.74      0.82      0.78        34\n",
      "          64       0.78      0.69      0.73        61\n",
      "          65       0.61      0.70      0.65        20\n",
      "          66       0.90      0.82      0.86       109\n",
      "          67       0.91      0.77      0.83        13\n",
      "          68       0.57      0.36      0.44        11\n",
      "          69       0.50      0.33      0.40         3\n",
      "          70       0.63      0.73      0.68        26\n",
      "          71       0.66      0.72      0.69        71\n",
      "          72       0.84      0.94      0.89        17\n",
      "          73       0.75      0.75      0.75         8\n",
      "          74       0.75      0.74      0.75        81\n",
      "          75       0.88      0.78      0.82        27\n",
      "          76       0.58      0.35      0.43        43\n",
      "          77       0.97      0.88      0.92        34\n",
      "          78       0.69      0.82      0.75        11\n",
      "          79       1.00      1.00      1.00        22\n",
      "          80       0.85      1.00      0.92        23\n",
      "          81       0.38      0.71      0.50         7\n",
      "          82       0.75      0.69      0.72        13\n",
      "          83       0.33      1.00      0.50         1\n",
      "          84       0.80      0.57      0.67         7\n",
      "          85       0.62      0.68      0.65        78\n",
      "          86       0.50      1.00      0.67         2\n",
      "          87       0.78      0.77      0.77        69\n",
      "          88       0.75      0.67      0.71         9\n",
      "          89       0.92      0.82      0.87        40\n",
      "          90       0.90      0.88      0.89        43\n",
      "          91       0.83      1.00      0.91         5\n",
      "          92       0.33      0.55      0.41        11\n",
      "          93       0.83      0.65      0.73        23\n",
      "          94       0.65      1.00      0.79        13\n",
      "          95       0.77      0.92      0.84        25\n",
      "          96       0.50      1.00      0.67         7\n",
      "          97       1.00      1.00      1.00         5\n",
      "          98       0.86      0.95      0.90        39\n",
      "          99       0.89      0.53      0.67       160\n",
      "         100       0.89      0.89      0.89         9\n",
      "         101       0.94      1.00      0.97        30\n",
      "         102       0.77      0.91      0.83        11\n",
      "         103       0.73      0.62      0.67       101\n",
      "         104       0.38      0.30      0.33        10\n",
      "         105       1.00      1.00      1.00         7\n",
      "         106       0.50      1.00      0.67         5\n",
      "         107       0.91      0.91      0.91        35\n",
      "         108       0.38      0.75      0.50         8\n",
      "         109       0.71      1.00      0.83         5\n",
      "         110       0.71      0.79      0.75        19\n",
      "         111       1.00      1.00      1.00         6\n",
      "         112       0.67      1.00      0.80        22\n",
      "         113       1.00      1.00      1.00         5\n",
      "         114       1.00      0.20      0.33         5\n",
      "         115       0.60      0.86      0.71         7\n",
      "         116       1.00      1.00      1.00         1\n",
      "         117       0.20      0.50      0.29         2\n",
      "         118       0.00      0.00      0.00         0\n",
      "         119       0.67      0.80      0.73        10\n",
      "         120       1.00      1.00      1.00         6\n",
      "         121       0.74      0.91      0.82        22\n",
      "         122       0.11      0.25      0.15         4\n",
      "         123       0.00      0.00      0.00         0\n",
      "         124       0.38      1.00      0.55         3\n",
      "         125       0.00      0.00      0.00         1\n",
      "         126       0.00      0.00      0.00         0\n",
      "         127       1.00      1.00      1.00         3\n",
      "         128       1.00      1.00      1.00         5\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       1.00      1.00      1.00         1\n",
      "         131       1.00      1.00      1.00         4\n",
      "         132       1.00      1.00      1.00         5\n",
      "\n",
      "    accuracy                           0.80      4530\n",
      "   macro avg       0.73      0.78      0.74      4530\n",
      "weighted avg       0.82      0.80      0.80      4530\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/mac/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "result = modelCNN_balence.predict(X_test_padded_seqs)  # 预测样本属于每个类别的概率\n",
    "Y_predict = np.argmax(result, axis=1)  # 获得最大概率对应的标签\n",
    "print('测试准确率', accuracy_score(Y_test, Y_predict))\n",
    "print('平均f1-score:', f1_score(Y_test, Y_predict, average='weighted'))\n",
    "print(classification_report(Y_predict,Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
